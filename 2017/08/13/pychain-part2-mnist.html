<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="shortcut icon" href="/media/img/favicon.png">
        <!-- SEO -->
        <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Pychain Part 2 - Application: MNIST | pvigier’s blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Pychain Part 2 - Application: MNIST" />
<meta name="author" content="pierre" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In part 1, we have created a fully functional library which is able to create and train neural networks using computational graphs. We used them on very simple examples. Today, we are going to try it on a more serious problem: character recognition. We are going to use a well-known database in the machine learning and deep learning world named MNIST. The database is available on Yann LeCun’s website. If you have read a bit about neural networks before you should have already seen his name. He is a French scientist who is one of the pioneers of neural networks and inventors of convolutional neural networks and he is now the director of AI at Facebook. Character recognition is an emblematic problem for two reasons. Firstly, it is one of the first successes and industrial applications of neural networks. It was used since the 90’s to read checks. Secondly, computer vision has always been a leading application domain for neural networks. In this part, we are going to briefly discover the MNIST database. Then, we are going to train some networks on it and finally, we are going to explore a bit how a neural network works." />
<meta property="og:description" content="In part 1, we have created a fully functional library which is able to create and train neural networks using computational graphs. We used them on very simple examples. Today, we are going to try it on a more serious problem: character recognition. We are going to use a well-known database in the machine learning and deep learning world named MNIST. The database is available on Yann LeCun’s website. If you have read a bit about neural networks before you should have already seen his name. He is a French scientist who is one of the pioneers of neural networks and inventors of convolutional neural networks and he is now the director of AI at Facebook. Character recognition is an emblematic problem for two reasons. Firstly, it is one of the first successes and industrial applications of neural networks. It was used since the 90’s to read checks. Secondly, computer vision has always been a leading application domain for neural networks. In this part, we are going to briefly discover the MNIST database. Then, we are going to train some networks on it and finally, we are going to explore a bit how a neural network works." />
<link rel="canonical" href="https://pvigier.github.io/2017/08/13/pychain-part2-mnist.html" />
<meta property="og:url" content="https://pvigier.github.io/2017/08/13/pychain-part2-mnist.html" />
<meta property="og:site_name" content="pvigier’s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-08-13T00:00:00+02:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Pychain Part 2 - Application: MNIST" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"pierre"},"description":"In part 1, we have created a fully functional library which is able to create and train neural networks using computational graphs. We used them on very simple examples. Today, we are going to try it on a more serious problem: character recognition. We are going to use a well-known database in the machine learning and deep learning world named MNIST. The database is available on Yann LeCun’s website. If you have read a bit about neural networks before you should have already seen his name. He is a French scientist who is one of the pioneers of neural networks and inventors of convolutional neural networks and he is now the director of AI at Facebook. Character recognition is an emblematic problem for two reasons. Firstly, it is one of the first successes and industrial applications of neural networks. It was used since the 90’s to read checks. Secondly, computer vision has always been a leading application domain for neural networks. In this part, we are going to briefly discover the MNIST database. Then, we are going to train some networks on it and finally, we are going to explore a bit how a neural network works.","@type":"BlogPosting","headline":"Pychain Part 2 - Application: MNIST","dateModified":"2017-08-13T00:00:00+02:00","datePublished":"2017-08-13T00:00:00+02:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://pvigier.github.io/2017/08/13/pychain-part2-mnist.html"},"url":"https://pvigier.github.io/2017/08/13/pychain-part2-mnist.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

        <!-- Bootstrap -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <!-- My themes -->
        <link href="/media/css/style.css" rel="stylesheet">
        <link href="/media/css/syntax.css" rel="stylesheet">
        <link href="/media/css/modal.css" rel="stylesheet">
        <!-- RSS -->
        <link type="application/atom+xml" rel="alternate" href="https://pvigier.github.io/rss.xml" title="pvigier's blog" />
    </head>
    <body>
        <div class="blog-masthead">
            <div class="container">
                <nav class="blog-nav">
    
        <a class="blog-nav-item" href="/">Blog</a>
    
        <a class="blog-nav-item" href="/articles/">Articles</a>
    
        <a class="blog-nav-item" href="/projects/">Projects</a>
    
        <a class="blog-nav-item" href="https://www.vagabondgame.com/">Vagabond</a>
    
        <a class="blog-nav-item" href="/about/">About</a>
    
</nav>
            </div>
        </div>
        <div class="container">
            <div class="blog-header">
                <h1 class="blog-title">pvigier's blog</h1>
                <p class="lead blog-description">computer science, programming and other ideas</p>
            </div>
            <div class="row">
                <div class="col-sm-8 blog-main">
                <div class="blog-post">
    <h2 class="blog-post-title">Pychain Part 2 - Application: MNIST</h2>
    <p class="blog-post-meta">13 Aug 2017 by <a href="/">pierre</a></p>
    <p>In <a href="/2017/07/21/pychain-part1-computational-graphs.html">part 1</a>, we have created a fully functional library which is able to create and train neural networks using computational graphs. We used them on very simple examples. Today, we are going to try it on a more serious problem: character recognition.</p>

<p>We are going to use a well-known database in the machine learning and deep learning world named MNIST. The database is available on <a href="http://yann.lecun.com/exdb/mnist/">Yann LeCun’s website</a>. If you have read a bit about neural networks before you should have already seen his name. He is a French scientist who is one of the pioneers of neural networks and inventors of convolutional neural networks and he is now the director of AI at Facebook.</p>

<p>Character recognition is an emblematic problem for two reasons. Firstly, it is one of the first successes and industrial applications of neural networks. It was used since the 90’s to read checks. Secondly, computer vision has always been a leading application domain for neural networks.</p>

<p>In this part, we are going to briefly discover the MNIST database. Then, we are going to train some networks on it and finally, we are going to explore a bit how a neural network works.</p>

<!--more-->

<h1 id="mnist-database">MNIST Database</h1>

<h2 id="get-the-dataset">Get the Dataset</h2>

<p>Firstly, you should download the four files named “train-images-idx3-ubyte.gz”, “train-labels-idx1-ubyte.gz”, “t10k-images-idx3-ubyte.gz”, “t10k-labels-idx1-ubyte.gz”. Then create a folder examples/mnist/mnist and uncompress them in the latter. If you have not already a file archiver, I can advise you to use <a href="http://www.7-zip.org/">7-zip</a> for Windows and <a href="https://itunes.apple.com/fr/app/the-unarchiver/id425424353?mt=12">The Unarchiver</a> for MacOS. As for Linux users, you should already have one.</p>

<p>You can read the full specification on the database’s page. But it is not necessary, I have already written a script to read the database for you. You should only know that the database is separated on two parts: the training set and the test set. The training set contains 60 000 labelled images and we are going to use it to train the network. The test set contains only 10 000 labelled images and we are going to use it only to test the network, not to train it. It is very important to keep separated the two parts otherwise, your accuracy would be largely overestimated.</p>

<p><img src="/media/img/part2/mnist.png" alt="Some examples of images in MNIST" class="center-image modal-image" /></p>

<h2 id="preprocessings">Preprocessings</h2>

<p>Firstly, before training our computational graphs, we should preprocess the data. In general, a good thing to do is to center the data. To be clear, if we have \(x^{(1)}, ..., x^{(N)} \in \mathbb{R}^n\) as data, the centered vectors are defined by:</p>

\[x^{(i)}_{c_j} = x^{(i)}_j - \mu_j\]

<p>with \(\mu_j = \frac{1}{N}\sum_{i=1}^{N}{x^{(i)}_j}\).</p>

<p>Consequently, each dimension has its values centered around zero.</p>

<p>The other common preprocessing is to apply <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a>. PCA gives an orthonormal basis in which the dimensions are decorrelated. If we note \(V\) the matrix which transforms the standard basis to this orthonormal basis. The new input vectors are:</p>

\[x^{(i)}_{d_j} = x^{(i)}_{c_j}V\]

<p>We left multiply because we have adopted a row convention.</p>

<p>Finally, the last common preprocessing is to equalize the covariance. For doing this, we divide each dimension by its standard deviation:</p>

\[\tilde{x}^{(i)}_j = \frac{x^{(i)}_{d_j}}{\sigma_j}\]

<p>with \(\sigma_j^2 = \frac{1}{N}\sum_{i=1}^{N}{(x^{(i)}_{d_j})^2}\).</p>

<p>Dividing by the standard deviation scales the values so that their order of magnitude is about 1.</p>

<p>On the image below, you can see the effects of these three preprocessings. Notice the different effects of each of them.</p>

<p><img src="/media/img/part2/preprocessings.png" alt="Visualization of the preprocessings" class="center-image modal-image" /></p>

<p>You might ask why it is useful to preprocess the data before training. I think there are at least two reasons.</p>

<p>The first one is as there is no dominant or privileged dimension in the data, it can speed up the training. I think it is not totally obvious to see how the shape of the dataset can modify the shape of the cost function and help the gradient descent algorithm. So, I made a little simulation. I created a very simple dataset where the inputs have only one dimension with a mean of 2 and a standard deviation of 3. The outputs are given by \(f : x \mapsto 3x + 1\). Then I chose a very simple model, the linear regression, with only two parameters so that we can easily visualize the cost function. Finally, I chose the mean squared error as cost function.</p>

<p><img src="/media/img/part2/cost_function.gif" alt="Effect of preprocessings on the cost function" class="center-image modal-image" /></p>

<p>Above, you can see that the shapes of the cost functions are very different. There is a dominant direction when the data are not preprocessed. Then you can see on the animation, the effect of the presence of a dominant direction on gradient descent. The algorithm tends to oscillate and does not go directly to the minimum.</p>

<p>If you want to see the code for the preprocessings and the animation of gradient descent, you can find it <a href="https://github.com/pvigier/preprocessings">here</a>.</p>

<p>The other advantage, is that when the data are normalized, it is easier to compare the parameters and the results with other problems. Many of the thumb rules for setting the hyperparameters such as the learning rates assume that the data are preprocessed.</p>

<p>In our case, we won’t use all these fancy things. We will keep things simple and only cancel the mean and divide by 255 to have the values in \([-0.5, 0.5]\). A reason why we don’t divide by the standard deviation is that some pixels are white in all the images and consequently their standard deviation is equal to zero.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">preprocess_dataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">mean_X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">if</span> <span class="n">mean</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">mean</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mean_X</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span><span class="p">,</span> <span class="n">mean_X</span></code></pre></figure>

<p>In the function <code class="language-plaintext highlighter-rouge">normalized_dataset</code>, <code class="language-plaintext highlighter-rouge">mean</code> is an optional parameter because we want to compute the mean for the training set, but we want to apply the same preprocessings the test set.</p>

<p>Below, you can find all the steps to prepare the dataset.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Prepare dataset
</span><span class="n">X</span><span class="p">,</span> <span class="p">(</span><span class="n">nb_rows</span><span class="p">,</span> <span class="n">nb_columns</span><span class="p">),</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">get_training_set</span><span class="p">(</span><span class="s">'examples/mnist/mnist'</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">mean</span> <span class="o">=</span> <span class="n">preprocess_dataset</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">shuffle_dataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">ohe_Y</span> <span class="o">=</span> <span class="n">one_hot_encode</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

<span class="n">X_test</span><span class="p">,</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">get_test_set</span><span class="p">(</span><span class="s">'examples/mnist/mnist'</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">preprocess_dataset</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">shuffle_dataset</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span></code></pre></figure>

<p>First, we retrieve the images thanks to the functions in mnist.py. Then we preprocess the dataset and shuffle it. Finally for the training, we create a one-hot encoded matrix for the labels called <code class="language-plaintext highlighter-rouge">ohe_Y</code>.</p>

<p>Our neural network won’t output a class directly. Instead it will output the probability distribution \(p(y \mid x)\) where \(x\) is the input. Concretely, for MNIST, the network will output a vector of \(\mathbb{R}^{10}\) which represents \((p(y=0 \mid x), \ldots, p(y=9 \mid x))\). So the network will learn to map an input to a distribution probability. Thus, our target outputs should also be a distribution probability. Consequently, we transform \(y^{(i)}\) to \(y^{(i)}_{ohe}\) according to:</p>

\[\begin{array}{rcl}
    y^{(i)}_{ohe} = (0, \ldots, 0, &amp; 1 &amp; , 0, \ldots, 0) \\
    &amp; \uparrow &amp; \\
    &amp; y^{(i)^{th}} &amp; element \\
\end{array}\]

<p>(with the convention that the \(0^{th}\) element is the first one)</p>

<p>For instance, \(3\) will be mapped to \((0, 0, 0, 1, 0, 0, 0, 0, 0, 0)\) and \(9\) to \((0, 0, 0, 0, 0, 0, 0, 0, 0, 1)\).</p>

<p>That’s all for the dataset and preprocessings. We are now ready to train some neural networks!</p>

<h1 id="training">Training</h1>

<p>Let’s start by creating a very simple network with only one layer of 10 neurons.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Create the graph
</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">nb_times_dataset</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">create_fully_connected_network</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span></code></pre></figure>

<p>The function <code class="language-plaintext highlighter-rouge">create_fully_connected_network</code> is very similar to the one of the previous part. I didn’t paste the code, you can retrieve it <a href="https://github.com/pvigier/pychain-part2">there</a> in examples/mnist. But I add a little diagram to visualize what the function is doing:</p>

<p><img src="/media/img/part2/graph_mnist.svg" alt="Graph used for MNIST" class="center-image modal-image" /></p>

<p>As you can see, the function creates a multilayer network where the hidden layers use \(\tanh\) as activation function and the output layer uses \(softmax\) to create a distribution probability. Finally, the cost function is included in the graph and it is a categorical cross entropy. You can learn more about the output functions and cost functions in the corresponding appendix.</p>

<p>The function <code class="language-plaintext highlighter-rouge">train_and_monitor</code> train the graph, it is also similar to the code of the previous part but a little more complete.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">train_and_monitor</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">t</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">accuracies_training</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">accuracies_test</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">i_batch</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Optimization algorithm
</span>    <span class="n">sgd</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span><span class="n">graph</span><span class="p">.</span><span class="n">get_parameter_nodes</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_times_dataset</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="c1"># Train
</span>            <span class="n">graph</span><span class="p">.</span><span class="n">propagate</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]])</span>
            <span class="n">cost</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="n">backpropagate</span><span class="p">([</span><span class="n">ohe_Y</span><span class="p">[</span><span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]])</span>
            <span class="n">sgd</span><span class="p">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="c1"># Monitor
</span>            <span class="n">i_batch</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'pass: {}/{}, batch: {}/{}, cost: {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">nb_times_dataset</span><span class="p">,</span> \
                    <span class="n">j</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">cost</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">monitor</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i_batch</span> <span class="o">%</span> <span class="mi">256</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">t</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">i_batch</span><span class="p">)</span>
                <span class="n">accuracies_training</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span>
                <span class="n">accuracies_test</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">))</span></code></pre></figure>

<p>There is a parameter <code class="language-plaintext highlighter-rouge">monitor</code> which allows to enable monitoring. If it is activated, the accuracy on the training and the test set will be computed periodically. Then at the end of the training, the learning curves will be shown. It is very practical to analyze the dynamic of training or to check if overfitting occurred.</p>

<p>All that remains to do is calling the function! With this architecture, you should obtain about 91.5% of accuracy with only one pass over the dataset and reach more than 92% with more passes. One pass takes a little less than 3s on my computer. I find the performance pretty decent!</p>

<p>To modify the architecture, you just have to change the value of the variable <code class="language-plaintext highlighter-rouge">layer</code>. For instance, to use a 3 layers neural network whose layers have respectively a size of 128, 64 and 10, you just have to replace <code class="language-plaintext highlighter-rouge">layers = [10]</code> by <code class="language-plaintext highlighter-rouge">layers = [128, 64, 10]</code>. After 10 passes, these architectures achieve 97.8% of accuracy. It is an incredible result for our little library!</p>

<h1 id="interpretation">Interpretation</h1>

<p>Our networks achieve great results, but can we go further and try to understand a bit how they work? We will explore two ways of understanding them. Firstly, we are going to try to understand the role of the weights and we will conjecture that neural networks combine elementary features to create complex ones and finally take a decision. Then, we will see that indeed, neural networks create features that are discriminating and allows to take a decision.</p>

<h2 id="weights">Weights</h2>

<p>The first layer of neurons directly takes the intensities of pixels as inputs. Consequently, it is possible to visualize the weights and to interpret them.</p>

<p>If \(W\) is the weights matrix of the first layer which contains \(m\) neurons then the \(j^{th}\) column corresponds to the \(j^{th}\) neuron and the coefficient \(w_{ij}\) to the weight of neuron \(j\) associated to pixel \(i\). Thus, it is possible to reshape the \(j^{th}\) column into an image and see the importance of each pixel for each neuron. The image below shows clearly how to interpret the weights matrix:</p>

<p><img src="/media/img/part2/weights_visualization.svg" alt="Interpretation of the weights matrix of the first layer" class="center-image modal-image" /></p>

<p>If we take a one layer neural network, there are 10 neurons which corresponds respectively to the digits 0, 1, …, 9. So the first neuron must find weights that determines well what is a zero, the second what is a one, …</p>

<p>Let’s call the function <code class="language-plaintext highlighter-rouge">display_weights</code> which reshape the weights and plot them see to check our hypothesis:</p>

<p><img src="/media/img/part2/weights_10.png" alt="Weights of the first layer for one layer network" class="center-image modal-image" /></p>

<p>Very cool! We can clearly see that the weights of each neuron represent a sort of pattern of each digit. The network learns the discriminating shapes and parts of each digits.</p>

<p>Now what happens for a multilayer network? When they are several layers the neurons of the first layer do not correspond to any digit. Their outputs are used by the next layers and it is the neurons of the last layer which correspond to specific digits.</p>

<p>Let’s see the weights of the first layer of a multilayer network [64, 32, 16, 10]:</p>

<p><img src="/media/img/part2/weights_64_32_16_10.png" alt="Weights of the first layer for a multilayer network" class="center-image modal-image" /></p>

<p>It is way more difficult to see digits in the weights. However, we can see small areas which are really bright of dark. These small areas correspond to small shapes which are discriminating.</p>

<p>For example, on the image of the 5th row and 7th column we can see a bright area which is looking for the presence of black pixels there. This neuron will output a positive value for digits where these pixels are black. This is the case for 8 and 6. On the contrary, It will output negative values for a 3 or a 9. Consequently, it allows to discriminate the digits with a closed loop at the bottom and those which have not got one.</p>

<p>The next layers will then combine these <em>basic features</em> to create more complex ones. And finally, the last layer will take these <em>deep features</em> created by the first layers as input to take the final decision.</p>

<h2 id="deep-features">Deep Features</h2>

<p>Let’s look a bit more at these deep features.</p>

<p>A multilayer neural network can be divided in two parts. The first layers which create the deep features and the last layer which gives the output. However, it is important to remark that the last layer is simply a linear classifier.</p>

<p><img src="/media/img/part2/deep_features.svg" alt="Decomposition of a neural networks" class="center-image modal-image" /></p>

<p>The linear classifiers are really simple to analyze and understand. We will consequently study them in order to understand a bit more the deep features.</p>

<h3 id="linear-classifiers">Linear Classifiers</h3>

<p>We will describe two types of linear classifier: the binary linear classifier and the multiclass linear classifier.</p>

<p>The output of a binary linear classifier is a single real number given by:</p>

\[f(x)=\sigma(w^Tx)\]

<p>Then the chosen class is \(g(x) = \left\{\begin{array}{rl}1 &amp; \text{ if } f(x) \geq 0.5 \\ 0 &amp; otherwise \end{array}\right.\).</p>

<p>And the output of multiclass is a probability distribution stored in a vector where the \(k^{th}\) coordinate is given by:</p>

\[f(x)_k=softmax(x)_k=\frac{\exp(w_k^Tx)}{\sum_{j}{\exp(w_j^Tx)}}\]

<p>Then the chosen class is \(g(x) = \underset{k}{argmax}(f(x)_k)\).</p>

<p>By the way they are called linear classifiers because they use linear combinations of the input to compute the output.</p>

<p>What it is interesting to know for a classifier is the shape of its <em>frontiers</em>. Let \(A_k = \{ x \in \mathcal{X}, g(x) = k \}\), it is the subset of the input space for which the output class is \(k\), we will call \(A_k\) the <em>decision area</em> for class \(k\). The frontiers are the areas which separate an \(A_i\) from another \(A_j\).</p>

<p>Let’s determine the decision areas and the frontiers. In order to do that, I have written a small program which randomly creates a linear classifier and plots the decision areas. You can get it <a href="https://github.com/pvigier/linear-classifiers">here</a>. Here are the results for a binary classifier and a multiclass classifier:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Binary</th>
      <th style="text-align: center">Multiclass</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/media/img/part2/binary_classifier.png" alt="Frontier of a binary linear classifier" class="center-image modal-image" /></td>
      <td style="text-align: center"><img src="/media/img/part2/multiclass_classifier.png" alt="Frontiers of a multiclass linear classifier" class="center-image modal-image" /></td>
    </tr>
  </tbody>
</table>

<p>For a binary linear classifier, the input space is divided in two half-spaces by a hyperplane, which is a line in 2D.</p>

<p>For a multiclass linear classifier, the input space is divided in cells which are intersection of half-spaces. Concretely, the cells have their sides which are flat.</p>

<p>If you want to formally prove these results, you can look at the exercises at the end of this part.</p>

<p>What we have seen is that if we want a linear classifier to correctly classify our inputs, it should be possible to separate the different classes with hyperplanes. We often say that the classes should be <em>linearly separable</em></p>

<p>Generally, the classes are not linearly separable so the accuracy of linear classifiers is not great. That’s a reason why we use neural networks to create non-linear functions which are able to model non-linear frontiers.</p>

<p>This is a point of view, now we can see another one. Multilayer neural networks try to find a transformation from the input space \(\mathcal{X}\) where the classes are not linearly separable to a space of deep features where they are.</p>

<p>In the next parts, we will show several examples where neural networks are indeed able to create deep features that make classes linearly separable or almost.</p>

<h3 id="deep-features-in-xor-and-the-disk">Deep Features in XOR and the Disk</h3>

<p>Do you remember the two examples of the previous part? They were very simple problems where the frontiers were non-linear. Hence, they are perfect candidates to see if the deep features are linearly separable.</p>

<p>I have slightly modified the code which creates the computational graph in order to have access to the output of the last layer.</p>

<p>In addition, I have added the function <code class="language-plaintext highlighter-rouge">display_deep_features</code> which displays the deep features. I copy the code below:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">display_deep_features</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="c1"># Compute the deep features
</span>    <span class="n">features</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="n">propagate</span><span class="p">([</span><span class="n">X</span><span class="p">])[</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># PCA with 2 components
</span>    <span class="n">features</span> <span class="o">-=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">svd</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    <span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">V</span><span class="p">.</span><span class="n">T</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">])</span>
    <span class="c1"># Plot points for each digit
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">Y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,:].</span><span class="n">T</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">'o'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Deep features'</span><span class="p">)</span></code></pre></figure>

<p>It computes the deep features for at most 1000 points. Note that we select the second output of the graph with <code class="language-plaintext highlighter-rouge">[1]</code> which is the output of the penultimate layer.</p>

<p>Then it applies PCA to project the deep features into the 2D space. The deep features space could have a high dimension so we must project them in 2D to be able to visualize them. Recall that PCA finds a good subspace in which to project to loss the minimum of information.</p>

<p>Finally, we plot the projected deep features.</p>

<p>Here are the results for XOR:</p>

<p><img src="/media/img/part2/xor_deep_features.png" alt="Deep features for XOR" class="center-image modal-image" /></p>

<p>The four points of the dataset which were not linearly separable now clearly are.</p>

<p>And here are two different results for the disk problem:</p>

<p><img src="/media/img/part2/disk_deep_features1.png" alt="Deep features for the disk" class="center-image modal-image" />
<img src="/media/img/part2/disk_deep_features2.png" alt="Deep features for the disk" class="center-image modal-image" /></p>

<p>These are great figures! In both cases, the deep features make the dataset linearly separable.</p>

<p>On these simple problems, neural networks successfully transform the inputs to deep features where classes are linearly separable.</p>

<h3 id="deep-features-in-mnist">Deep Features in MNIST</h3>

<p>Let’s do the same experiment on MNIST. I use the network [128, 64, 10] which gives very good accuracy and I obtain this result:</p>

<p><img src="/media/img/part2/mnist_deep_features.png" alt="Deep features for MNIST in 2D" class="center-image modal-image" /></p>

<p>The points of a same color are clearly gathered on a same region of the space, but it is clearly not linearly separable.</p>

<p>The first explanation is that the network does not obtain an accuracy of 100% but only 97.8% so the deep features should only be almost linearly separable. However, this does not completely explain this messy result.</p>

<p>The other explanation is that the deep features are almost linearly separable in their space which has here 64 dimensions. And when we project the deep features in 2D we totally loose the linear separability.</p>

<p>Let’s try to project them in the 3D space in order to retain more information:</p>

<p><img src="/media/img/part2/mnist_deep_features_3d.gif" alt="Deep features for MNIST in 3D" class="center-image modal-image" /></p>

<p>That’s a lot better. The classes are more separated in the space.</p>

<h1 id="conclusion">Conclusion</h1>

<p>I hope that you earn a bit of intuition on neural networks through these experiments.</p>

<p>To conclude this part, I am going to sum up the different results we have seen:</p>

<ul>
  <li>The first layer of a multilayer neural network creates <em>basic features</em> from the data which are then combined by other layers to create more complex features.</li>
  <li>A neural network can be divided in two parts: a part which create <em>deep features</em> and another part which is a linear classifier and uses the deep features to give an output.</li>
  <li>During training, a neural network looks for a good transformation which makes the deep features of the dataset linearly separable.</li>
</ul>

<h1 id="to-go-further">To Go Further</h1>

<p>Some problems to have fun:</p>
<ul>
  <li>Try to find the best architecture and parameters to achieve the highest accuracy on the test set.
<a href="#clue1" data-toggle="collapse">Clues</a></li>
</ul>
<div id="clue1" class="collapse">
<p>On his <a href="http://yann.lecun.com/exdb/mnist/index.html">page</a> Yann LeCun reports results for different models including feedforward neural networks. It can give you some ideas for the architectures. My best accuracy is 97.8%, try to beat it! You will surely overfit, maybe you can look at regularization to improve your results.</p>

<p>If you achieve a better result, send me a mail or leave a comment with your parameters and your best accuracy. I will update the record and credit you.</p>
</div>
<ul>
  <li>The shape in the weights are a bit noisy. Add weight decay in the model and see how it smoothes the shapes. Explain the phenomenon.
<a href="#clue2" data-toggle="collapse">Clues</a></li>
</ul>
<div id="clue2" class="collapse">
<p>There are several ways to add weight decay: by modifying the graph or by modifying the optimization algorithm. The second solution is the fastest to implement, the update rule for weights is:</p>

$$
\theta_{t+1} = (1-\lambda)\theta_t - \eta \frac{\partial J}{\partial \theta}(\theta_t)
$$

<p>where \(\lambda\) is the regularization rate. You can get the code in the part about optimization algorithms.</p>

<p>Here is my weights for a network with only one layer and 0.01 as regularization rate:</p>

<p><img src="/media/img/part2/weights_10_smoothed.png" alt="Weights of the first layer for one layer network with regularization" class="center-image modal-image" /></p>

<p>The images are a lot smoother, especially the background.</p>

<p>I have no good explanation for this phenomenon at the moment. If you find one please send me a mail!</p>
</div>
<ul>
  <li>Proof that a binary linear classifier separates the space in two half-spaces and that a multiclass linear classifier separates the space in cells which are intersection of half-spaces.
<a href="#clue3" data-toggle="collapse">Clues</a></li>
</ul>
<div id="clue3" class="collapse">
<p>Let's denote \(f\) the output of the classifier and \(A_k\) the points of space that belong to class k according to the classifier.</p>

<p>For a binary linear classifier, we have \(f(x)=\sigma(w^Tx)\) and \(A_1 = \{x \in \mathcal{X}, f(x) \geq 0.5\}\) but \(\sigma(w^Tx) \geq 0.5\) is equivalent to \(w^Tx \geq 0.5\). \(w^Tx = 0.5\) is the equation of a hyperplane so \(A_1 = \{x \in \mathcal{X}, w^Tx \geq 0.5\}\) contains all the points which are on one side of the hyperlane. \(A_0\) contains the other half-space.</p>

<p>For a multiclass linear classifier, we have \(f(x)_k=softmax(x)_k=\frac{\exp(w_k^Tx)}{\sum_{j}{\exp(w_j^Tx)}}\) and \(A_k = \{x \in \mathcal{X}, k = argmax_j(f(x)_j)\}\). The denominator of softmax is the same for all \(j\) so it does not matter for determining the maximum. Thus \(argmax_j(f(x)_j) = argmax_j(\exp(w_j^Tx)) = argmax_j(w_j^Tx)\) because \(\exp\) is increasing.

Now, let's work a bit with the definition of argmax and inequalities to get the result:</p>

$$
\begin{array}{rcl}
A_k &amp; = &amp; \{x \in \mathcal{X}, k = argmax_j(w_j^Tx)\} \\
&amp; = &amp; \{x \in \mathcal{X}, \forall j \neq k, w_k^Tx \geq w_j^Tx)\} \\
&amp; = &amp; \{x \in \mathcal{X}, w_k^Tx \geq w_1^Tx, \ldots, w_k^Tx \geq w_K^Tx)\} \\
&amp; = &amp; \cap_{j \neq k}{x \in \mathcal{X}, \{w_k^Tx \geq w_j^Tx\}} \\
&amp; = &amp; \cap_{j \neq k}{x \in \mathcal{X}, \{(w_k-w_j)^Tx \geq 0\}} \\
\end{array}
$$

The last line is exactly what we wanted.
</div>

    <p><em>If you are interested in my adventures during the development of <a href="https://www.vagabondgame.com">Vagabond</a>, you can follow me on <a href="https://twitter.com/PierreVigier">Twitter</a>.</em></p>
    
	<p>Tags: <span class="label label-primary"><a href="/tag/math">math</a></span> <span class="label label-primary"><a href="/tag/python">python</a></span> </p>
	
</div><!-- /.blog-post -->
<hr/>
<p>Subscribe to the newsletter if you do not want to miss any new article:</p>
<!-- Begin Mailchimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
	#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; width:100%;}
	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="https://ymail.us20.list-manage.com/subscribe/post?u=7bb3b720a12ef1d8e0b48d8da&amp;id=7516dd4562" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">

	<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_7bb3b720a12ef1d8e0b48d8da_7516dd4562" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>
<!--End mc_embed_signup-->
<!-- Disqus -->
<hr/>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://pvigier-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                </div>
                <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
    <p><strong><a href="https://store.steampowered.com/app/1673090/Vagabond/">Wishlist Vagabond on Steam!</a></strong></p>
	<div class="sidebar-module">
        <h4>Tags</h4>
        <ol class="list-unstyled">
		
			<li><a href="/tag/math">math (3)</a></li>
		
			<li><a href="/tag/python">python (6)</a></li>
		
			<li><a href="/tag/cpp">cpp (8)</a></li>
		
			<li><a href="/tag/pcg">pcg (15)</a></li>
		
			<li><a href="/tag/simulopolis">simulopolis (5)</a></li>
		
			<li><a href="/tag/linux">linux (1)</a></li>
		
			<li><a href="/tag/geometry">geometry (1)</a></li>
		
			<li><a href="/tag/graph">graph (1)</a></li>
		
			<li><a href="/tag/git">git (1)</a></li>
		
			<li><a href="/tag/vagabond">vagabond (26)</a></li>
		
			<li><a href="/tag/ecs">ecs (2)</a></li>
		
			<li><a href="/tag/game-engine">game-engine (8)</a></li>
		
        </ol>
    </div>
    <div class="sidebar-module">
        <h4>Archives</h4>
        <ol class="list-unstyled">
		
        
            
            
            
            
			<li id="yMarch 2021"><a href="/2021/03">March 2021 (1)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
			<li id="yDecember 2020"><a href="/2020/12">December 2020 (2)</a></li>
				
            
        
            
            
            
            
			<li id="yMarch 2020"><a href="/2020/03">March 2020 (1)</a></li>
				
            
        
            
            
            
            
			<li id="yFebruary 2020"><a href="/2020/02">February 2020 (1)</a></li>
				
            
        
            
            
            
            
			<li id="yNovember 2019"><a href="/2019/11">November 2019 (1)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
			<li id="yOctober 2019"><a href="/2019/10">October 2019 (2)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
			<li id="ySeptember 2019"><a href="/2019/09">September 2019 (2)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
			<li id="yAugust 2019"><a href="/2019/08">August 2019 (4)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
			<li id="yJuly 2019"><a href="/2019/07">July 2019 (5)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
			<li id="yJune 2019"><a href="/2019/06">June 2019 (4)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
			<li id="yMay 2019"><a href="/2019/05">May 2019 (5)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
			<li id="yNovember 2018"><a href="/2018/11">November 2018 (3)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
			<li id="yOctober 2018"><a href="/2018/10">October 2018 (3)</a></li>
				
            
        
            
            
            
            
			<li id="yJune 2018"><a href="/2018/06">June 2018 (1)</a></li>
				
            
        
            
            
            
            
			<li id="yMay 2018"><a href="/2018/05">May 2018 (1)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
			<li id="yFebruary 2018"><a href="/2018/02">February 2018 (2)</a></li>
				
            
        
            
            
            
            
			<li id="yAugust 2017"><a href="/2017/08">August 2017 (1)</a></li>
				
            
        
            
            
            
            
			<li id="yJuly 2017"><a href="/2017/07">July 2017 (1)</a></li>
				
            
        
            
            
            
            
			<li id="yFebruary 2017"><a href="/2017/02">February 2017 (1)</a></li>
				
            
        
        </ol>
    </div>
    <div class="sidebar-module">
        <h4>Follow me</h4>
        <ol class="list-unstyled">
            <li><a href="https://github.com/pvigier">GitHub</a></li>
            <li><a href="https://pvigier.itch.io/">itch.io</a></li>
            <li><a href="https://twitter.com/PierreVigier">Twitter</a></li>
            <li><a href="/rss.xml">RSS</a></li>
        </ol>
    </div>
    <div class="sidebar-module">
        <a class="twitter-timeline" data-height="400" href="https://twitter.com/PierreVigier?ref_src=twsrc%5Etfw">Tweets by PierreVigier</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
    </div>
</div><!-- /.blog-sidebar -->

            </div><!-- /.row -->
        </div><!-- /.container -->

        <footer class="blog-footer">
            <p>Powered by <a href="http://getbootstrap.com">Bootstrap</a> and <a href="http://jekyllrb.com">Jekyll</a>.</p>
            <p>
                <a href="#">Back to top</a>
            </p>
        </footer>
        <!-- Javascript -->
        <!-- jQuery -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
        <!-- Bootstrap -->
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
        <!-- MathJax -->
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
        <!-- Analytics -->
        <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-30902264-4', 'auto');
        ga('send', 'pageview');
        </script>
        <!-- Modal images -->
        <script src="/media/js/modal.js"></script>
    </body>
</html>
