<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="shortcut icon" href="/media/img/favicon.png">
        <!-- SEO -->
        <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Pychain Part 1 - Computational Graphs | pvigier’s blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Pychain Part 1 - Computational Graphs" />
<meta name="author" content="pierre" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Welcome in this big tutorial on neural networks! Our goal is to write our own deep learning framework like TensorFlow or Torch. We are going to learn in-depth how neural networks work, all the mechanics behind them. We will get our hands dirty and code everything! In this tutorial, we will use Python3 and scipy but I hope that the code and the ideas are clear enough so that you can adapt the code to your favorite language. First, I show you the plan. In this part, we are going to quickly introduce neural networks and then, we will introduce computational graphs in order to model them. In the end of this part, we are going to use our implementation to learn some non-linear functions. In the second part, we will deal with a more serious problem. We are going to build an optical character recognition system upon the MNIST database. It is a classical problem in machine learning, we have to do it. Then, we will tackle recurrent neural networks and show how to model them with our library. To apply our new knowledge, we will try to learn a formal grammar generated by an automaton. In part 4, we will go further with recurrent neural networks and introduce the well-known LSTM cell. We will briefly compare it with fully-connected recurrent neural networks. To approach part 6, some more efficient optimization algorithms are necessary. Consequently, we will discuss them in part 5. Have you ever read this fabulous article by Andrej Karpathy? Yes? Cool, because, we are going to reproduce his results with our own library in part 6. Amazing, isn’t it? Finally, parts 7 and 8 are going to be theoretical appendices for the most curious readers. Is it all? Maybe not! Stay tuned! If you are ready, let’s go!" />
<meta property="og:description" content="Welcome in this big tutorial on neural networks! Our goal is to write our own deep learning framework like TensorFlow or Torch. We are going to learn in-depth how neural networks work, all the mechanics behind them. We will get our hands dirty and code everything! In this tutorial, we will use Python3 and scipy but I hope that the code and the ideas are clear enough so that you can adapt the code to your favorite language. First, I show you the plan. In this part, we are going to quickly introduce neural networks and then, we will introduce computational graphs in order to model them. In the end of this part, we are going to use our implementation to learn some non-linear functions. In the second part, we will deal with a more serious problem. We are going to build an optical character recognition system upon the MNIST database. It is a classical problem in machine learning, we have to do it. Then, we will tackle recurrent neural networks and show how to model them with our library. To apply our new knowledge, we will try to learn a formal grammar generated by an automaton. In part 4, we will go further with recurrent neural networks and introduce the well-known LSTM cell. We will briefly compare it with fully-connected recurrent neural networks. To approach part 6, some more efficient optimization algorithms are necessary. Consequently, we will discuss them in part 5. Have you ever read this fabulous article by Andrej Karpathy? Yes? Cool, because, we are going to reproduce his results with our own library in part 6. Amazing, isn’t it? Finally, parts 7 and 8 are going to be theoretical appendices for the most curious readers. Is it all? Maybe not! Stay tuned! If you are ready, let’s go!" />
<link rel="canonical" href="https://pvigier.github.io/2017/07/21/pychain-part1-computational-graphs.html" />
<meta property="og:url" content="https://pvigier.github.io/2017/07/21/pychain-part1-computational-graphs.html" />
<meta property="og:site_name" content="pvigier’s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-07-21T00:00:00+02:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Pychain Part 1 - Computational Graphs" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"pierre"},"headline":"Pychain Part 1 - Computational Graphs","dateModified":"2017-07-21T00:00:00+02:00","datePublished":"2017-07-21T00:00:00+02:00","description":"Welcome in this big tutorial on neural networks! Our goal is to write our own deep learning framework like TensorFlow or Torch. We are going to learn in-depth how neural networks work, all the mechanics behind them. We will get our hands dirty and code everything! In this tutorial, we will use Python3 and scipy but I hope that the code and the ideas are clear enough so that you can adapt the code to your favorite language. First, I show you the plan. In this part, we are going to quickly introduce neural networks and then, we will introduce computational graphs in order to model them. In the end of this part, we are going to use our implementation to learn some non-linear functions. In the second part, we will deal with a more serious problem. We are going to build an optical character recognition system upon the MNIST database. It is a classical problem in machine learning, we have to do it. Then, we will tackle recurrent neural networks and show how to model them with our library. To apply our new knowledge, we will try to learn a formal grammar generated by an automaton. In part 4, we will go further with recurrent neural networks and introduce the well-known LSTM cell. We will briefly compare it with fully-connected recurrent neural networks. To approach part 6, some more efficient optimization algorithms are necessary. Consequently, we will discuss them in part 5. Have you ever read this fabulous article by Andrej Karpathy? Yes? Cool, because, we are going to reproduce his results with our own library in part 6. Amazing, isn’t it? Finally, parts 7 and 8 are going to be theoretical appendices for the most curious readers. Is it all? Maybe not! Stay tuned! If you are ready, let’s go!","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://pvigier.github.io/2017/07/21/pychain-part1-computational-graphs.html"},"url":"https://pvigier.github.io/2017/07/21/pychain-part1-computational-graphs.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

        <!-- Bootstrap -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <!-- My themes -->
        <link href="/media/css/style.css" rel="stylesheet">
        <link href="/media/css/syntax.css" rel="stylesheet">
        <link href="/media/css/modal.css" rel="stylesheet">
        <!-- RSS -->
        <link type="application/atom+xml" rel="alternate" href="https://pvigier.github.io/rss.xml" title="pvigier's blog" />
    </head>
    <body>
        <div class="blog-masthead">
            <div class="container">
                <nav class="blog-nav">
    
        <a class="blog-nav-item" href="/">Blog</a>
    
        <a class="blog-nav-item" href="/articles/">Articles</a>
    
        <a class="blog-nav-item" href="/projects/">Projects</a>
    
        <a class="blog-nav-item" href="https://www.vagabondgame.com/">Vagabond</a>
    
        <a class="blog-nav-item" href="/about/">About</a>
    
</nav>
            </div>
        </div>
        <div class="container">
            <div class="blog-header">
                <h1 class="blog-title">pvigier's blog</h1>
                <p class="lead blog-description">computer science, programming and other ideas</p>
            </div>
            <div class="row">
                <div class="col-sm-8 blog-main">
                <div class="blog-post">
    <h2 class="blog-post-title">Pychain Part 1 - Computational Graphs</h2>
    <p class="blog-post-meta">21 Jul 2017 by <a href="/">pierre</a></p>
    <p>Welcome in this big tutorial on neural networks!</p>

<p>Our goal is to write our own deep learning framework like TensorFlow or Torch. We are going to learn in-depth how neural networks work, all the mechanics behind them.</p>

<p>We will get our hands dirty and code everything! In this tutorial, we will use Python3 and scipy but I hope that the code and the ideas are clear enough so that you can adapt the code to your favorite language.</p>

<p>First, I show you the plan. In this part, we are going to quickly introduce neural networks and then, we will introduce computational graphs in order to model them. In the end of this part, we are going to use our implementation to learn some non-linear functions.</p>

<p>In the second part, we will deal with a more serious problem. We are going to build an optical character recognition system upon the <a href="http://yann.lecun.com/exdb/mnist/">MNIST database</a>. It is a classical problem in machine learning, we have to do it.</p>

<p>Then, we will tackle recurrent neural networks and show how to model them with our library. To apply our new knowledge, we will try to learn a formal grammar generated by an automaton.</p>

<p>In part 4, we will go further with recurrent neural networks and introduce the well-known LSTM cell. We will briefly compare it with fully-connected recurrent neural networks.</p>

<p>To approach part 6, some more efficient optimization algorithms are necessary. Consequently, we will discuss them in part 5.</p>

<p>Have you ever read <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">this fabulous article</a> by Andrej Karpathy? Yes? Cool, because, we are going to reproduce his results with our own library in part 6. Amazing, isn’t it?</p>

<p>Finally, parts 7 and 8 are going to be theoretical appendices for the most curious readers.</p>

<p>Is it all? Maybe not! Stay tuned!</p>

<p>If you are ready, let’s go!
<!--more--></p>

<h1 id="neural-networks">Neural Networks</h1>

<p>I am not going to make a long presentation on neural networks. Why? Because there are already a lot of good pages on the web about them.</p>

<p>If you want a gentle introduction to them, you can read <a href="http://neuralnetworksanddeeplearning.com/">these pages</a> by Michael Nielsen. It is well written and easy to follow.</p>

<p>If you want a more academic and in-depth text on neural networks, I must advise you to read the amazing <a href="http://www.deeplearningbook.org/">Deep Learning Book</a> (aka The book). I learnt a lot from this book. Lots of ideas that I will present in this tutorial are inspired by it.</p>

<p>During the redaction of these articles, I discovered <a href="http://cs231n.github.io/">this course</a> from Stanford. I encourage you to have a look on it!</p>

<p>Another reason why I do not present neural networks, is that we will adopt a modern approach on them. We are not going to see a neural network as a network of neurons but instead we will consider it as a parametric function.</p>

<h1 id="machine-learning">Machine Learning</h1>

<h2 id="the-learning-problem">The Learning Problem</h2>

<p>Let’s speak a bit of machine learning, more precisely of supervised learning. In supervised learning, there is an unknown function \(g : \mathcal{X} \rightarrow \mathcal{Y}\) and we have a dataset of examples \(D = ((x_1, y_1), ..., (x_N, y_N))\) such that \(\forall (x, y) \in D, y = g(x)\). The goal is to use the dataset \(D\) to reconstruct \(g\). In other words, we want to find a function \(f\) such that:</p>

\[\forall x \in \mathcal{X}, f(x) \approx g(x)\]

<p>To go further we have two issues to tackle. The first one is that we have not access to the value of \(g\) for all \(x\) in \(\mathcal{X}\), only those in \(D\). Consequently, we will use a method called <em>empirical risk minimization</em> (ERM). Intuitively, we will try to have \(\forall (x, y) \in D, y \approx f(x)\) and hope that it works well for the other values.</p>

<p>Then we need to formalize a bit the approximation symbol \(\approx\). To do that, we will use a cost function, we will note it \(J\). There are a variety of different cost functions, a famous one is the quadratic cost:</p>

\[J(f, D) = \sum_{(x, y) \in D}{||y - f(x)||_2^2}\]

<p>If you want to know more about cost functions, how to derive them from statistics, regularization, etc. You are welcome to read the appendix on cost functions.</p>

<p>Now that we have a cost function, our goal is to find a function \(f\) such that \(J(f, D)\) is small.</p>

<p>We still have a big problem. What is \(f\)? We could try to find the best \(f\) in the whole space of functions but it is not a good idea because there is an infinity of functions which have a cost equal to zero and most of them generalize very badly. We will instead restrict ourselves to a specific class of smoother functions and for that we are going to use, like I said before, parametric functions. A parametric function is simply a function \(f_{\theta}\) that depends on a parameter \(\theta\).</p>

<p>The problem is now to find the best possible \(\theta\) to fit our data. Formally, we will try to solve the following problem:</p>

\[\theta^* = \underset{\theta}{argmin}J(f_{\theta}, D))\]

<p>Such a problem is called an <em>optimization problem</em> and there exist good tools to tackle it.</p>

<h2 id="gradient-descent">Gradient Descent</h2>

<p>I am going to quickly present an algorithm to find a good \(\theta\). The algorithm is called <em>gradient descent</em>. It consists of choosing randomly an initial parameter \(\theta_0\) and at each step of the algorithm we will optimize locally to improve the solution. The idea is to make small steps to diminish the cost. We have to decide in which direction we make these steps.</p>

<p>Thanks to <a href="https://en.wikipedia.org/wiki/Taylor%27s_theorem">Taylor’s theorem</a>, we know that around a point \(x_0\), a differentiable function \(f\) can be approximated by:</p>

\[f(x) \approx f(x_0) + \frac{\partial f}{\partial x}(x_0)^T(x - x_0)\]

<p>Maybe you are more familiar with the 1D case which says that around a point a differentiable function can be approximated by its tangent:</p>

\[f(x) \approx f(x_0) + f'(x_0)(x - x_0)\]

<p>Let’s visualize what this formula means and how it can be useful to find a good direction. The generalization of the tangent, which is a line, in a vector space is a hyperplane. In a 2D space it corresponds to a plane.</p>

<p><img src="/media/img/part1/tangent_space.png" alt="Taylor's theorem visualization" class="center-image modal-image" /></p>

<p>The plane in blue is the approximation given by Taylor’s theorem. The points in orange are the outputs of \(f\) for all points at a same distance \(d\) to \(x_0\). In green, it’s the output of the point at distance \(d\) of \(x_0\) in the direction of the gradient and in blue, it’s the output of the point at distance $d$ in the opposite direction of the gradient.</p>

<p>You can clearly see that the direction toward which the plane goes up the quickest is the gradient \(\frac{\partial f}{\partial x}(x_0)\). Conversely, the direction in which it goes down the quickest is the opposite of the gradient \(-\frac{\partial f}{\partial x}(x_0)\).</p>

<p>If you are more a math person who likes to play with formulas, you can remark that \(\frac{\partial f}{\partial x}(x_0)^T(x - x_0)\) is an inner product. And, if we consider all points at the same distance \(d\) to \(x_0\), the <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Cauchy-Schwarz inequality</a> tells us that this term is maximized when the point \((x - x_0)\) has the same direction as the gradient and is minimized when the point has the opposite direction of the gradient.</p>

<p>Let’s go back to our goal to minimize the cost. Let \(\theta_t\) be the value of the parameter at iteration \(t\) of the algorithm. We can see \(J\) as a function of \(\theta\), and we want to find a point around \(\theta_t\) that has a lower cost. Using our first order approximation of \(J\) around \(\theta_t\) we know that the direction toward which the cost is decreasing the most is \(-\frac{\partial J}{\partial \theta}(\theta_t)\). Consequently, we will make a small step in this direction and choose the point:</p>

\[\theta_{t+1} = \theta_t - \eta \frac{\partial J}{\partial \theta}(\theta_t)\]

<p>where \(\eta\) is called the <em>learning rate</em>. It is a parameter that controls the size of the steps we make. We will dig deeper into optimization algorithms later in another part.</p>

<p>You can see gradient descent in action, in the animation below.</p>

<p><img src="/media/img/part1/gradient_descent.gif" alt="Gradient descent in action" class="center-image modal-image" /></p>

<p>You can get the code of these two simulations <a href="https://github.com/pvigier/gradient-descent">here</a>.</p>

<p>I stop there for the brief introduction to machine learning. We will now see how to model some parametric functions and how they relate to neural networks.</p>

<h1 id="computational-graphs">Computational Graphs</h1>

<p>A <em>computational graph</em> is a graph which represents a computation.</p>

<p>Let’s see some examples to show how this tool can be used to model useful functions.</p>

<p>Imagine, you want to fit very simple data like in the figure below.</p>

<p><img src="/media/img/part1/linear_regression.png" alt="Linear regression" class="center-image modal-image" /></p>

<p>The natural idea is to use a function like this one \(f_{\theta} : x \mapsto ax + b\) where \(\theta = (a, b)\) is the parameter of the function. This function can be represented with the following computational graph.</p>

<p><img src="/media/img/part1/linear_regression_graph.svg" alt="Computational graph for the linear regression" class="center-image modal-image" /></p>

<p>Now, imagine you have to solve a more difficult problem and you want to use a neural network, like the one below.</p>

<p><img src="/media/img/part1/network_3_5_3.svg" alt="Neural networks 3-5-3" class="center-image modal-image" /></p>

<p>It has two layers, the hidden layer uses \(\tanh\) as activation function and the output layer uses the sigmoid \(\sigma : t \mapsto \frac{1}{1 + \exp(-t)}\).</p>

<p>This model is strictly equivalent to the function \(f_{\theta} : x \mapsto \sigma(\tanh(xW_1)W_2)\) where \(W_1\) are the weights of the first layer and \(W_2\) the weights of the second layer. In this case again, \(f\) is a parametric function which depends on \(\theta = (W_1, W_2)\).</p>

<p>The following computational graph corresponds to the neural network depicted above.</p>

<p><img src="/media/img/part1/graph_3_5_3.svg" alt="Computational graph for the neural networks" class="center-image modal-image" /></p>

<p>We can see several things from these examples. Firstly there are different nodes:</p>
<ul>
  <li>Input nodes (in rose): they represent the inputs of the function. In our examples, they are the \(x\) nodes.</li>
  <li>Operation nodes (in blue): they are nodes that represent operations. They take inputs, make a computation and give ouputs. In our examples, the \(\sigma\), \(\tanh\), \(\times\), \(+\) nodes are operation nodes.</li>
  <li>Parameter nodes (in green): they represent the parameters of the function. In our example, they are \(a\), \(b\), \(W_1\) and \(W_2\).</li>
</ul>

<p>Secondly, it allows a more compact notation, instead of drawing lots of neurons, we can only draw few nodes to represent a whole network.</p>

<p>Finally, we can model a broader class of parametric functions. We are not limited at all by the biological inspiration. You can unleash your engineering spirit and craft all sort of functions.</p>

<p>In the next sections, we will explain how to code a computational graph to model a parametric function and how to optimize them to fit your data.</p>

<h1 id="architecture-of-the-library">Architecture of the Library</h1>

<p>First, I am going to present the architecture of the library. Then we are going to explain the code.</p>

<p>There are three main classes that we are going to code in this part: the Node class, the Graph class and the OptimizationAlgorithm class.</p>

<p>The Node class contains most of the logic of the library. It’s the hard part.</p>

<p>The Graph class is a container that contains nodes and allows to interact easily with them.</p>

<p>Finally, the OptimizationAlgorithm class implements an optimization algorithm like the gradient descent algorithm we have described above. The class uses the parameter nodes and optimizes their value to minimize the cost. This class will allow us to easily switch between different algorithms. If you are familiar with design patterns, you should have recognized the <a href="https://en.wikipedia.org/wiki/Strategy_pattern">strategy pattern</a>.</p>

<p>The UML diagram below gives a global view of the classes and their interactions.</p>

<p><img src="/media/img/part1/uml_diagram.svg" alt="UML diagram of the library" class="center-image modal-image" /></p>

<p>And before, I forget you can retrieve the full code for this chapter <a href="https://github.com/pvigier/pychain-part1">here</a>.</p>

<h1 id="node-class">Node Class</h1>

<p>A node have several inputs and several outputs. During the propagation, a node uses its inputs to compute its ouputs as depicted below.</p>

<p><img src="/media/img/part1/node_propagation.svg" alt="Propagation in a node" class="center-image modal-image" /></p>

<p>The goal of the backpropagation is to compute the derivative of the cost with respect to the parameters to use the gradient descent algorithm. To do that, we are going to use the <em>chain rule</em>, a lot. Indeed, thanks to the chain rule, it is possible to express the derivative of the cost with respect to an input of a node with the derivatives of the cost with respect to the outputs of the same node.</p>

<p><img src="/media/img/part1/node_backpropagation.svg" alt="Backpropagation in a node" class="center-image modal-image" /></p>

<p>Precisely, if a node has \(m\) inputs and \(n\) outputs, we have:</p>

\[\forall i \in \{1, ..., m\}, \frac{\partial J}{\partial x_i} = \sum_{j=1}^{n}{\frac{\partial y_j}{\partial x_i}\frac{\partial J}{\partial y_j}}\]

<p>And the term \(\frac{\partial y_j}{\partial x_i}\) only depends of the nature of the node.</p>

<p>Consequently, we are going to backpropagate the derivatives from the “end” of the graph to the parameter nodes.</p>

<p>An illustration with the neural networks shown before:</p>

<p><img src="/media/img/part1/graph_backpropagation.svg" alt="Backpropagation in a graph" class="center-image modal-image" /></p>

<h2 id="base-class">Base Class</h2>

<p>Most of the code is contained in the base class. We are going to divide the work in two stages. First, we are going to see the methods that are useful to create the computational graph. Then we are going to describe the methods used during the propagation and the backpropagation.</p>

<p>Let’s see the first part of the code.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Node</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parents</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">nb_outputs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1"># Parents for each input
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">set_parents</span><span class="p">(</span><span class="n">parents</span> <span class="ow">or</span> <span class="p">[])</span>
        <span class="c1"># Children for each output
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">children</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_outputs</span><span class="p">)]</span>

        <span class="c1"># Memoization
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dJdx</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dJdy</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="c1"># Dirty flags
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">output_dirty</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gradient_dirty</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="k">def</span> <span class="nf">set_parents</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parents</span><span class="p">):</span>
        <span class="c1"># Fill self.parents
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">parents</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i_input</span><span class="p">,</span> <span class="p">(</span><span class="n">parent</span><span class="p">,</span> <span class="n">i_parent_output</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parents</span><span class="p">):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">parents</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">parent</span><span class="p">,</span> <span class="n">i_parent_output</span><span class="p">))</span>
            <span class="n">parent</span><span class="p">.</span><span class="n">add_child</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i_input</span><span class="p">,</span> <span class="n">i_parent_output</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_child</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">child</span><span class="p">,</span> <span class="n">i_child_input</span><span class="p">,</span> <span class="n">i_output</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">children</span><span class="p">[</span><span class="n">i_output</span><span class="p">].</span><span class="n">append</span><span class="p">((</span><span class="n">child</span><span class="p">,</span> <span class="n">i_child_input</span><span class="p">))</span></code></pre></figure>

<p>A node has a list of its parents, the list have the following form <code class="language-plaintext highlighter-rouge">[(node1, i_output1), (node2, i_output2), ... ]</code>. For each parent node, we precise the index of its output to which the current node is connected. The first couple corresponds to the first input of the node \(x_1\), the second to \(x_2\), etc.</p>

<p>A node has a 2D list of its children too. The first dimension is the output index. And for each output index, there is a list containing the nodes connected to this output. The list has the following form <code class="language-plaintext highlighter-rouge">[(node1, i_input1), (node2, i_input2), ...]</code>. For each child node, we precise the index of its input that is connected to this output.</p>

<p>OK, it is a bit hard to follow. Hopefully, a diagram will make it crystal clear!</p>

<p><img src="/media/img/part1/node_links.svg" alt="Node links" class="center-image modal-image" /></p>

<p>We can set the parents during the initialization or later by using <code class="language-plaintext highlighter-rouge">set_parents</code>. We have not to manually set the children.</p>

<p>That’s all for the dependencies between nodes.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Node</span><span class="p">:</span>
   <span class="c1"># ...
</span>
   <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i_output</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">output_dirty</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">parent</span><span class="p">.</span><span class="n">get_output</span><span class="p">(</span><span class="n">i_parent_output</span><span class="p">)</span> \
                <span class="k">for</span> <span class="p">(</span><span class="n">parent</span><span class="p">,</span> <span class="n">i_parent_output</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">parents</span><span class="p">]</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">compute_output</span><span class="p">()</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">output_dirty</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">[</span><span class="n">i_output</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">compute_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i_input</span><span class="p">):</span>
        <span class="c1"># If there are no children, return zero
</span>        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">children</span><span class="p">)</span> <span class="k">for</span> <span class="n">children</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">children</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">i_input</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span>
        <span class="c1"># Get gradient with respect to the i_inputth input
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">gradient_dirty</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dJdy</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">child</span><span class="p">.</span><span class="n">get_gradient</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> \
                <span class="k">for</span> <span class="n">child</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">children</span><span class="p">)</span> <span class="k">for</span> <span class="n">children</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">children</span><span class="p">]</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">dJdx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">compute_gradient</span><span class="p">()</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">gradient_dirty</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">dJdx</span><span class="p">[</span><span class="n">i_input</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_memoization</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Reset flags
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">output_dirty</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gradient_dirty</span> <span class="o">=</span> <span class="bp">True</span></code></pre></figure>

<p>The implementation of propagation and backpropagation is very symmetrical.</p>

<p>The method <code class="language-plaintext highlighter-rouge">get_output</code> asks the parents of the node their outputs and save them in <code class="language-plaintext highlighter-rouge">self.x</code>. Then we use, <code class="language-plaintext highlighter-rouge">compute_output</code> and we save the result in <code class="language-plaintext highlighter-rouge">self.y</code>. Finally, we set the dirty flag to false. We use <a href="https://en.wikipedia.org/wiki/Memoization">memoization</a> not to compute twice the same value. It is very important because each child will ask the node for outputs, and recursively this node will ask its parents for their outputs, etc. If we do not use memoization, we would waste a lot of time doing unnecessary computations.</p>

<p>The method <code class="language-plaintext highlighter-rouge">compute_output</code> is abstract, it will be implemented in specialized classes.</p>

<p><code class="language-plaintext highlighter-rouge">get_gradient</code> is very similar to <code class="language-plaintext highlighter-rouge">get_output</code>, for each output we gather the derivative of the cost with respect to this output and save it in <code class="language-plaintext highlighter-rouge">self.dJdy</code>. Then we use <code class="language-plaintext highlighter-rouge">compute_gradient</code> to compute <code class="language-plaintext highlighter-rouge">self.dJdx</code> and set the dirty flag to false.</p>

<p>Finally, there is the method <code class="language-plaintext highlighter-rouge">reset_memoization</code> to set the flags to true.</p>

<p>In the next subsections, we are going to see the specializations of this class.</p>

<h2 id="input-nodes">Input Nodes</h2>

<p>The class <code class="language-plaintext highlighter-rouge">InputNode</code> has an attribute <code class="language-plaintext highlighter-rouge">value</code> that can be set. The function <code class="language-plaintext highlighter-rouge">get_output</code> is overloaded to directly return this value.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">InputNode</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">Node</span><span class="p">.</span><span class="n">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">set_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">value</span>

    <span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">dJdy</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span></code></pre></figure>

<h2 id="parameter-nodes">Parameter Nodes</h2>

<p>The class <code class="language-plaintext highlighter-rouge">ParameterNode</code> has a new parameter <code class="language-plaintext highlighter-rouge">w</code>. We use the letter <code class="language-plaintext highlighter-rouge">w</code> to refer to the weights, like in neural networks.</p>

<p>The output of the node is simply the weight.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">ParameterNode</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
        <span class="n">Node</span><span class="p">.</span><span class="n">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">w</span>

    <span class="k">def</span> <span class="nf">compute_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">w</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">dJdy</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span></code></pre></figure>

<p>This class is very simple. The only issue is that the constructor expects an initial value for the weights. There are several thumb rules regarding the initialization of the weights. The general idea is to sample from a zero mean distribution with a small standard deviation. You can see more details about initialization in <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Efficient BackProp</a>.</p>

<p>You will see several examples of initialization later in the tutorial.</p>

<h2 id="operation-nodes">Operation Nodes</h2>

<p>Then there are the operation nodes. There is a lot of implemented nodes, I am not going to describe the code for each of them.</p>

<p>Let’s take the example of the <code class="language-plaintext highlighter-rouge">SigmoidNode</code> which implements the function \(\sigma\) element-wise applied.</p>

<p>We only plug the formulas for the output and the gradient in the methods <code class="language-plaintext highlighter-rouge">get_output</code> and <code class="language-plaintext highlighter-rouge">get_gradient</code>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">SigmoidNode</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">compute_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span>

    <span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">dJdy</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span></code></pre></figure>

<p>To have decent performances in Python, it is very important to use numpy at most and to forbid for-loops. Consequently, it is important to express the formulas in term of matrix operations.</p>

<p>If you want to see all the formulas and how to derive the backpropagation formulas, you can go to the appendix on matrix derivations.</p>

<h2 id="gradient-nodes">Gradient Nodes</h2>

<p>The last type of node is the <code class="language-plaintext highlighter-rouge">GradientNode</code>. I have never spoken about gradient node yet. There are very similar to input nodes but this time, it is the value of the gradient that can be set.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">GradientNode</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parents</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">Node</span><span class="p">.</span><span class="n">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parents</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">set_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">compute_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">x</span>

    <span class="k">def</span> <span class="nf">get_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i_input</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">value</span></code></pre></figure>

<p>During backpropagation, if every node asks their children for gradient, to whom the “last” node asks its gradient with respect to its output? Gradient nodes are the answer to this question. We can set the gradient to be backpropagated.</p>

<p>There are two ways of doing this. We can just add a gradient node as a child to the output node of the graph. Then, we set manually the value of the gradient node to the derivative of the cost with respect to the output of the graph.</p>

<p>Another way of proceeding that I prefer is to add the cost function directly in the computational graph. Then, we just have to add a gradient node with a value of \(1\) as a child of the cost node. And that’s it!</p>

<p>Consider the graph representing the neural networks described above. If we had an input node \(y\) to represent the expected output and we choose the squared error as cost function, we obtain this new graph:</p>

<p><img src="/media/img/part1/graph_with_cost.svg" alt="A graph with cost inside it" class="center-image modal-image" /></p>

<p>The yellow node at the end is a gradient node which always returns 1 as gradient.</p>

<h1 id="graph-class">Graph Class</h1>

<p>The <code class="language-plaintext highlighter-rouge">Graph</code> class is very simple.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Graph</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nodes</span><span class="p">,</span> <span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">expected_output_nodes</span><span class="p">,</span> <span class="n">cost_node</span><span class="p">,</span> <span class="n">parameter_nodes</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">nodes</span> <span class="o">=</span> <span class="n">nodes</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_nodes</span> <span class="o">=</span> <span class="n">input_nodes</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">output_nodes</span> <span class="o">=</span> <span class="n">output_nodes</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">expected_output_nodes</span> <span class="o">=</span> <span class="n">expected_output_nodes</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cost_node</span> <span class="o">=</span> <span class="n">cost_node</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">parameter_nodes</span> <span class="o">=</span> <span class="n">parameter_nodes</span>

        <span class="c1"># Create a gradient node
</span>        <span class="n">GradientNode</span><span class="p">([(</span><span class="n">cost_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">propagate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reset_memoization</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_nodes</span><span class="p">):</span>
            <span class="n">node</span><span class="p">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">node</span><span class="p">.</span><span class="n">get_output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">output_nodes</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">backpropagate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">y</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">expected_output_nodes</span><span class="p">):</span>
            <span class="n">node</span><span class="p">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cost_node</span><span class="p">.</span><span class="n">get_output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">parameter_nodes</span><span class="p">:</span>
            <span class="n">node</span><span class="p">.</span><span class="n">get_gradient</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cost</span>

    <span class="k">def</span> <span class="nf">reset_memoization</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">nodes</span><span class="p">:</span>
            <span class="n">node</span><span class="p">.</span><span class="n">reset_memoization</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_parameter_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">parameter_nodes</span></code></pre></figure>

<p>It takes the list of all the nodes in the graph. In addition, it needs to know the function of several nodes in the graph: the inputs, the ouputs, the expected outputs and the cost.</p>

<p>There are also two useful functions:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">propagate</code>: to compute the outputs of the graph ;</li>
  <li><code class="language-plaintext highlighter-rouge">backpropagate</code>: to accumulate the gradient of the cost with respect to the parameters.</li>
</ul>

<p>Finally, the method <code class="language-plaintext highlighter-rouge">reset_memoization</code> resets the flags for all the nodes and <code class="language-plaintext highlighter-rouge">get_parameter_nodes</code> returns the parameter nodes. The latter method is useful to give to the optimization algorithm the nodes for which it has to optimize the weights.</p>

<h1 id="optimization-algorithms">Optimization Algorithms</h1>

<p>The last class we have to write before we can test our library is the <code class="language-plaintext highlighter-rouge">OptimizationAlgorithm</code> class.</p>

<p>The class is a generic template for all first-order iterative optimization algorithms such as gradient descent.</p>

<p>For this kind of algorithm at each step \(t\), we compute a direction \(v_t\) toward which we should move the parameter \(\theta_t\). We have:</p>

\[\theta_{t+1} = \theta_t - \eta v_t\]

<p>Where \(\eta\) is the learning rate.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">OptimizationAlgorithm</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameter_nodes</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">parameter_nodes</span> <span class="o">=</span> <span class="n">parameter_nodes</span>

        <span class="c1"># Parameters
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">parameter_nodes</span><span class="p">):</span>
            <span class="n">direction</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">compute_direction</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">node</span><span class="p">.</span><span class="n">get_gradient</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="n">node</span><span class="p">.</span><span class="n">w</span> <span class="o">-=</span> <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">direction</span>

    <span class="k">def</span> <span class="nf">compute_direction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">()</span></code></pre></figure>

<p>For gradient descent as explained before, we simply have \(v_t = \frac{\partial J}{\partial \theta}(\theta_t)\).</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">GradientDescent</span><span class="p">(</span><span class="n">OptimizationAlgorithm</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameter_nodes</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="n">OptimizationAlgorithm</span><span class="p">.</span><span class="n">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameter_nodes</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">compute_direction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">grad</span></code></pre></figure>

<h1 id="some-examples">Some Examples</h1>

<p>Now we have a fully functional library. The last missing piece is to create a computational graph and try it!</p>

<p>We are going to use fully-connected neural networks to learn two functions respectively XOR and a disk.</p>

<p>We will use the function <code class="language-plaintext highlighter-rouge">fully_connected</code> which takes as input a list of the sizes of the layers and returns a computational graph which represents such a network.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">create_fully_connected_network</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">parameter_nodes</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Input
</span>    <span class="n">input_node</span> <span class="o">=</span> <span class="n">InputNode</span><span class="p">()</span>
    <span class="n">nodes</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_node</span><span class="p">)</span>

    <span class="n">cur_input_node</span> <span class="o">=</span> <span class="n">input_node</span>
    <span class="n">prev_size</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
        <span class="c1"># Create a layer
</span>        <span class="n">bias_node</span> <span class="o">=</span> <span class="n">AddBiasNode</span><span class="p">([(</span><span class="n">cur_input_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)])</span>
        <span class="n">parameter_node</span> <span class="o">=</span> <span class="n">ParameterNode</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">prev_size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">prod_node</span> <span class="o">=</span> <span class="n">MultiplicationNode</span><span class="p">([(</span><span class="n">bias_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">parameter_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)])</span>
        <span class="c1"># Activation function for hidden layers
</span>        <span class="k">if</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
            <span class="c1"># Tanh
</span>            <span class="n">activation_node</span> <span class="o">=</span> <span class="n">TanhNode</span><span class="p">([(</span><span class="n">prod_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)])</span>
            <span class="c1"># Relu
</span>            <span class="c1">#activation_node = ReluNode([(prod_node, 0)])
</span>        <span class="c1"># Activation function for the output layer
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">activation_node</span> <span class="o">=</span> <span class="n">SigmoidNode</span><span class="p">([(</span><span class="n">prod_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)])</span>
        <span class="c1"># Save the new nodes
</span>        <span class="n">parameter_nodes</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">parameter_node</span><span class="p">)</span>
        <span class="n">nodes</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias_node</span><span class="p">,</span> <span class="n">parameter_node</span><span class="p">,</span> <span class="n">prod_node</span><span class="p">,</span> <span class="n">activation_node</span><span class="p">]</span>
        <span class="n">cur_input_node</span> <span class="o">=</span> <span class="n">activation_node</span>
        <span class="n">prev_size</span> <span class="o">=</span> <span class="n">size</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="c1"># Expected output
</span>    <span class="n">expected_output_node</span> <span class="o">=</span> <span class="n">InputNode</span><span class="p">()</span>
    <span class="c1"># Cost function
</span>    <span class="n">cost_node</span> <span class="o">=</span> <span class="n">BinaryCrossEntropyNode</span><span class="p">([(</span><span class="n">expected_output_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">cur_input_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)])</span>

    <span class="n">nodes</span> <span class="o">+=</span> <span class="p">[</span><span class="n">expected_output_node</span><span class="p">,</span> <span class="n">cost_node</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">Graph</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="p">[</span><span class="n">input_node</span><span class="p">],</span> <span class="p">[</span><span class="n">cur_input_node</span><span class="p">],</span> <span class="p">[</span><span class="n">expected_output_node</span><span class="p">],</span> <span class="n">cost_node</span><span class="p">,</span> <span class="n">parameter_nodes</span><span class="p">)</span></code></pre></figure>

<p>The function is a bit long but simple, we simply create nodes and connect them. First, we create an input node. Then, for each layer, we create a bias node, a parameter node, a multiplication node and an activation node. These nodes model the operation \(x \mapsto f((1 \mid x)W)\) where \((1 \mid x)\) denotes the matrix \(x\) to which we have added a column of 1 at the beginning, \(W\) the weights of the parameter node and \(f\) the activation function.</p>

<p>We use two different activation functions: \(\tanh\) and \(\sigma\). \(\tanh\) is used in the hidden layers, it is a nonlinear function which maps \(\mathbb{R}\) to \(]-1, 1[\). It is very important that activation functions are nonlinear otherwise the multilayer neural network is equivalent to a one layer neural network.</p>

<p>We use the sigmoid function as activation function for the output layer because it maps \(\mathbb{R}\) to \(]0,1[\) and consequently the output of the network can be interpreted as a probability, usually \(p(y=1 \mid x)\).</p>

<p>For the cost function, we choose the binary cross entropy. If you want to know more about the output functions and the cost functions, you can read the appendix.</p>

<h2 id="xor">XOR</h2>

<p>Learning XOR is an interesting problem because XOR is surely the simplest boolean function that cannot be learnt by a linear classifier. Crafted features or a deep neural network are necessary to solve this nonlinear problem.</p>

<p>Let’s use this piece of code to train the network.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Create the graph and initialize the optimization algorithm
</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">create_fully_connected_network</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
<span class="n">sgd</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span><span class="n">graph</span><span class="p">.</span><span class="n">get_parameter_nodes</span><span class="p">(),</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># Train
</span><span class="n">t</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">nb_passes</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="k">for</span> <span class="n">i_pass</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_passes</span><span class="p">):</span>
    <span class="c1"># Propagate, backpropagate and optimize
</span>    <span class="n">graph</span><span class="p">.</span><span class="n">propagate</span><span class="p">([</span><span class="n">X</span><span class="p">])</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="n">backpropagate</span><span class="p">([</span><span class="n">Y</span><span class="p">])</span> <span class="o">/</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">sgd</span><span class="p">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1"># Save the cost
</span>    <span class="n">t</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">i_pass</span><span class="p">)</span>
    <span class="n">costs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span></code></pre></figure>

<p>Finally, you can use the <code class="language-plaintext highlighter-rouge">visualize</code> to compute the output of the graph for every \(x \in [-0.5, 1.5]^2\) and see the frontier. You can see that the network models a nonlinear function.</p>

<p>Below, you can see such generated images for two different activation functions for the hidden layers. We can note that \(\tanh\) seems to create smooth frontiers while \(ReLU\) creates sharper ones.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">With tanh</th>
      <th style="text-align: center">With ReLU</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/media/img/part1/xor_4_4_1_tanh.png" alt="XOR with tanh as activation function" class="center-image modal-image" /></td>
      <td style="text-align: center"><img src="/media/img/part1/xor_4_4_1_relu.png" alt="XOR with ReLU as activation function" class="center-image modal-image" /></td>
    </tr>
  </tbody>
</table>

<h2 id="disk">Disk</h2>

<p>Just for fun let’s learn another function and generate beautiful images. The function we want to learn is such that \(f(x) = 1\) if \(x\) is inside the disk of radius 0.5 and centered on (0.5, 0.5) and \(f(x) = 0\) otherwise.</p>

<p>The function is clearly nonlinear and consequently, it is a perfect problem for a neural network.</p>

<p>To test the code for the disk function, you just have to comment/uncomment few lines in train.py. Below, there are some outputs.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">With tanh</th>
      <th style="text-align: center">With ReLU</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/media/img/part1/disk_4_4_1_tanh.png" alt="XOR with tanh as activation function" class="center-image modal-image" /></td>
      <td style="text-align: center"><img src="/media/img/part1/disk_4_4_1_relu.png" alt="XOR with ReLU as activation function" class="center-image modal-image" /></td>
    </tr>
  </tbody>
</table>

<h1 id="to-go-further">To Go Further</h1>

<p>Some problems to sharpen your understanding of neural networks:</p>
<ul>
  <li>What is the smallest number of layers and neurons necessary to perfectly learn the XOR function? Verify your answer by experience.
<a href="#clue1" data-toggle="collapse">Clues</a></li>
</ul>
<div id="clue1" class="collapse">
<p>As mentionned before, one layer (only the output layer) is not sufficient because the frontier would be a straight line. So we should take at least two layers. Each layer should have at least two neurons otherwise the frontier is again a straight line. A network with 2 layers and 2 neurons is the smallest network able to learn the XOR function. You could try to find acceptable weights by hand or you can use gradient descent!</p>
</div>
<ul>
  <li>Study the influence of the learning rate.
<a href="#clue2" data-toggle="collapse">Clues</a></li>
</ul>
<div id="clue2" class="collapse">
<p>Try very small values, values around 1 and very high values.</p>

<p>You can see on the curves below that the cost tends to zero for learning rates small enough. And the larger the learning rate is, the faster the cost tends to zero. However for too large learning rates the cost does not tend to zero anymore.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Convergence</th>
      <th style="text-align: center">Divergence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/media/img/part1/learning_rates_convergence.png" alt="Learning curves for low learning rates" class="center-image" /></td>
      <td style="text-align: center"><img src="/media/img/part1/learning_rates_divergence.png" alt="Learning curves for high learning rates" class="center-image" /></td>
    </tr>
  </tbody>
</table>

</div>
<ul>
  <li>Try to create other datasets using other functions and learn them using a computational graph.</li>
</ul>

    <p><em>If you are interested in my adventures during the development of <a href="https://www.vagabondgame.com">Vagabond</a>, you can follow me on <a href="https://twitter.com/PierreVigier">Twitter</a>.</em></p>
    
	<p>Tags: <span class="label label-primary"><a href="/tag/math">math</a></span> <span class="label label-primary"><a href="/tag/python">python</a></span> </p>
	
</div><!-- /.blog-post -->
<hr/>
<p>Subscribe to the newsletter if you do not want to miss any new article:</p>
<!-- Begin Mailchimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
	#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; width:100%;}
	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="https://ymail.us20.list-manage.com/subscribe/post?u=7bb3b720a12ef1d8e0b48d8da&amp;id=7516dd4562" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">

	<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_7bb3b720a12ef1d8e0b48d8da_7516dd4562" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>
<!--End mc_embed_signup-->
<!-- Disqus -->
<hr/>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://pvigier-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                </div>
                <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
    <p><strong><a href="https://store.steampowered.com/app/1673090/Vagabond/">Wishlist Vagabond on Steam!</a></strong></p>
	<div class="sidebar-module">
        <h4>Tags</h4>
        <ol class="list-unstyled">
		
			<li><a href="/tag/math">math (3)</a></li>
		
			<li><a href="/tag/python">python (6)</a></li>
		
			<li><a href="/tag/cpp">cpp (8)</a></li>
		
			<li><a href="/tag/pcg">pcg (15)</a></li>
		
			<li><a href="/tag/simulopolis">simulopolis (5)</a></li>
		
			<li><a href="/tag/linux">linux (1)</a></li>
		
			<li><a href="/tag/geometry">geometry (1)</a></li>
		
			<li><a href="/tag/graph">graph (1)</a></li>
		
			<li><a href="/tag/git">git (1)</a></li>
		
			<li><a href="/tag/vagabond">vagabond (26)</a></li>
		
			<li><a href="/tag/ecs">ecs (2)</a></li>
		
			<li><a href="/tag/game-engine">game-engine (8)</a></li>
		
        </ol>
    </div>
    <div class="sidebar-module">
        <h4>Archives</h4>
        <ol class="list-unstyled">
		
        
            
            
            
            
			<li id="yMarch 2021"><a href="/2021/03">March 2021 (1)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
			<li id="yDecember 2020"><a href="/2020/12">December 2020 (2)</a></li>
				
            
        
            
            
            
            
			<li id="yMarch 2020"><a href="/2020/03">March 2020 (1)</a></li>
				
            
        
            
            
            
            
			<li id="yFebruary 2020"><a href="/2020/02">February 2020 (1)</a></li>
				
            
        
            
            
            
            
			<li id="yNovember 2019"><a href="/2019/11">November 2019 (1)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
			<li id="yOctober 2019"><a href="/2019/10">October 2019 (2)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
			<li id="ySeptember 2019"><a href="/2019/09">September 2019 (2)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
			<li id="yAugust 2019"><a href="/2019/08">August 2019 (4)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
			<li id="yJuly 2019"><a href="/2019/07">July 2019 (5)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
			<li id="yJune 2019"><a href="/2019/06">June 2019 (4)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
			<li id="yMay 2019"><a href="/2019/05">May 2019 (5)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
			<li id="yNovember 2018"><a href="/2018/11">November 2018 (3)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
			<li id="yOctober 2018"><a href="/2018/10">October 2018 (3)</a></li>
				
            
        
            
            
            
            
			<li id="yJune 2018"><a href="/2018/06">June 2018 (1)</a></li>
				
            
        
            
            
            
            
			<li id="yMay 2018"><a href="/2018/05">May 2018 (1)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
			<li id="yFebruary 2018"><a href="/2018/02">February 2018 (2)</a></li>
				
            
        
            
            
            
            
			<li id="yAugust 2017"><a href="/2017/08">August 2017 (1)</a></li>
				
            
        
            
            
            
            
			<li id="yJuly 2017"><a href="/2017/07">July 2017 (1)</a></li>
				
            
        
            
            
            
            
			<li id="yFebruary 2017"><a href="/2017/02">February 2017 (1)</a></li>
				
            
        
        </ol>
    </div>
    <div class="sidebar-module">
        <h4>Follow me</h4>
        <ol class="list-unstyled">
            <li><a href="https://github.com/pvigier">GitHub</a></li>
            <li><a href="https://pvigier.itch.io/">itch.io</a></li>
            <li><a href="https://twitter.com/PierreVigier">Twitter</a></li>
            <li><a href="/rss.xml">RSS</a></li>
        </ol>
    </div>
    <div class="sidebar-module">
        <a class="twitter-timeline" data-height="400" href="https://twitter.com/PierreVigier?ref_src=twsrc%5Etfw">Tweets by PierreVigier</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
    </div>
</div><!-- /.blog-sidebar -->

            </div><!-- /.row -->
        </div><!-- /.container -->

        <footer class="blog-footer">
            <p>Powered by <a href="http://getbootstrap.com">Bootstrap</a> and <a href="http://jekyllrb.com">Jekyll</a>.</p>
            <p>
                <a href="#">Back to top</a>
            </p>
        </footer>
        <!-- Javascript -->
        <!-- jQuery -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
        <!-- Bootstrap -->
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
        <!-- MathJax -->
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
        <!-- Analytics -->
        <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-30902264-4', 'auto');
        ga('send', 'pageview');
        </script>
        <!-- Modal images -->
        <script src="/media/js/modal.js"></script>
    </body>
</html>
