<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="">
        <meta name="author" content="Pierre Vigier">

        <title>Pychain Part 1 - Computational graphs - pvigier's blog</title>
        <link rel="shortcut icon" href="/media/img/favicon.png">
        <!-- Bootstrap -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <!-- My themes -->
        <link href="/media/css/style.css" rel="stylesheet">
        <link href="/media/css/syntax.css" rel="stylesheet">
        <link href="/media/css/modal.css" rel="stylesheet">
    </head>
    <body>
        <div class="blog-masthead">
            <div class="container">
                <nav class="blog-nav">
                    <a class="blog-nav-item active" href="/">Blog</a>
                    <a class="blog-nav-item" href="/articles">Articles</a>
                    <a class="blog-nav-item" href="/games">Games</a>
                    <!--<a class="blog-nav-item" href="/best-resources">Best Resources</a>-->
                    <a class="blog-nav-item" href="/resume">Resume</a>
                    <a class="blog-nav-item" href="/about">About</a>
                </nav>
            </div>
        </div>
        <div class="container">
            <div class="blog-header">
                <h1 class="blog-title">pvigier's blog</h1>
                <p class="lead blog-description">computer science, programming and other ideas</p>
            </div>
            <div class="row">
            <div class="col-sm-8 blog-main">
<div class="blog-post">
    <h2 class="blog-post-title">Pychain Part 1 - Computational graphs</h2>
    <p class="blog-post-meta">21 Jul 2017 by <a href="/">pierre</a></p>
    <p>Welcome in this big tutorial on neural networks!</p>

<p>Our goal is to write our own deep learning framework like TensorFlow or Torch. We are going to learn in-depth how neural networks work, all the mechanics behind them.</p>

<p>We will get our hands dirty and code everything! In this tutorial, we will use Python3 and scipy but I hope that the code and the ideas are clear enough so that you can adapt the code to your favorite language.</p>

<p>First, I show you the plan. In this part, we are going to quickly introduce neural networks and then, we will introduce computational graphs in order to model them. In the end of this part, we are going to use our implementation to learn some non-linear functions.</p>

<p>In the second part, we will deal with a more serious problem. We are going to build an optical character recognition system upon the <a href="http://yann.lecun.com/exdb/mnist/">MNIST database</a>. It is a classical problem in machine learning, we have to do it.</p>

<p>Then, we will tackle recurrent neural networks and show how to model them with our library. To apply our new knowledge, we will try to learn a formal grammar generated by an automaton.</p>

<p>In part 4, we will go further with recurrent neural networks and introduce the well-known LSTM cell. We will briefly compare it with fully-connected recurrent neural networks.</p>

<p>To approach part 6, some more efficient optimization algorithms are necessary. Consequently, we will discuss them in part 5.</p>

<p>Have you ever read <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">this fabulous article</a> by Andrej Karpathy? Yes? Cool, because, we are going to reproduce his results with our own library in part 6. Amazing, isn’t it?</p>

<p>Finally, parts 7 and 8 are going to be theoretical appendices for the most curious readers.</p>

<p>Is it all? Maybe not! Stay tuned!</p>

<p>If you are ready, let’s go!
<!--more--></p>

<h1 id="neural-networks">Neural networks</h1>

<p>I am not going to make a long presentation on neural networks. Why? Because there are already a lot of good pages on the web about them.</p>

<p>If you want a gentle introduction to them, you can read <a href="http://neuralnetworksanddeeplearning.com/">these pages</a> by Michael Nielsen. It is well written and easy to follow.</p>

<p>If you want a more academic and in-depth text on neural networks, I must advise you to read the amazing <a href="http://www.deeplearningbook.org/">Deep Learning Book</a> (aka The book). I learnt a lot from this book. Lots of ideas that I will present in this tutorial are inspired by it.</p>

<p>During the redaction of these articles, I discovered <a href="http://cs231n.github.io/">this course</a> from Stanford. I encourage you to have a look on it!</p>

<p>Another reason why I do not present neural networks, is that we will adopt a modern approach on them. We are not going to see a neural network as a network of neurons but instead we will consider it as a parametric function.</p>

<h1 id="machine-learning">Machine Learning</h1>

<h2 id="the-learning-problem">The learning problem</h2>

<p>Let’s speak a bit of machine learning, more precisely of supervised learning. In supervised learning, there is an unknown function <script type="math/tex">g : \mathcal{X} \rightarrow \mathcal{Y}</script> and we have a dataset of examples <script type="math/tex">D = ((x_1, y_1), ..., (x_N, y_N))</script> such that <script type="math/tex">\forall (x, y) \in D, y = g(x)</script>. The goal is to use the dataset <script type="math/tex">D</script> to reconstruct <script type="math/tex">g</script>. In other words, we want to find a function <script type="math/tex">f</script> such that:</p>

<script type="math/tex; mode=display">\forall x \in \mathcal{X}, f(x) \approx g(x)</script>

<p>To go further we have two issues to tackle. The first one is that we have not access to the value of <script type="math/tex">g</script> for all <script type="math/tex">x</script> in <script type="math/tex">\mathcal{X}</script>, only those in <script type="math/tex">D</script>. Consequently, we will use a method called <em>empirical risk minimization</em> (ERM). Intuitively, we will try to have <script type="math/tex">\forall (x, y) \in D, y \approx f(x)</script> and hope that it works well for the other values.</p>

<p>Then we need to formalize a bit the approximation symbol <script type="math/tex">\approx</script>. To do that, we will use a cost function, we will note it <script type="math/tex">J</script>. There are a variety of different cost functions, a famous one is the quadratic cost:</p>

<script type="math/tex; mode=display">J(f, D) = \sum_{(x, y) \in D}{||y - f(x)||_2^2}</script>

<p>If you want to know more about cost functions, how to derive them from statistics, regularization, etc. You are welcome to read the appendix on cost functions.</p>

<p>Now that we have a cost function, our goal is to find a function <script type="math/tex">f</script> such that <script type="math/tex">J(f, D)</script> is small.</p>

<p>We still have a big problem. What is <script type="math/tex">f</script>? We could try to find the best <script type="math/tex">f</script> in the whole space of functions but it is not a good idea because there is an infinity of functions which have a cost equal to zero and most of them generalize very badly. We will instead restrict ourselves to a specific class of smoother functions and for that we are going to use, like I said before, parametric functions. A parametric function is simply a function <script type="math/tex">f_{\theta}</script> that depends on a parameter <script type="math/tex">\theta</script>.</p>

<p>The problem is now to find the best possible <script type="math/tex">\theta</script> to fit our data. Formally, we will try to solve the following problem:</p>

<script type="math/tex; mode=display">\theta^* = \underset{\theta}{argmin}J(f_{\theta}, D))</script>

<p>Such a problem is called an <em>optimization problem</em> and there exist good tools to tackle it.</p>

<h2 id="gradient-descent">Gradient descent</h2>

<p>I am going to quickly present an algorithm to find a good <script type="math/tex">\theta</script>. The algorithm is called <em>gradient descent</em>. It consists of choosing randomly an initial parameter <script type="math/tex">\theta_0</script> and at each step of the algorithm we will optimize locally to improve the solution. The idea is to make small steps to diminish the cost. We have to decide in which direction we make these steps.</p>

<p>Thanks to <a href="https://en.wikipedia.org/wiki/Taylor%27s_theorem">Taylor’s theorem</a>, we know that around a point <script type="math/tex">x_0</script>, a differentiable function <script type="math/tex">f</script> can be approximated by:</p>

<script type="math/tex; mode=display">f(x) \approx f(x_0) + \frac{\partial f}{\partial x}(x_0)^T(x - x_0)</script>

<p>Maybe you are more familiar with the 1D case which says that around a point a differentiable function can be approximated by its tangent:</p>

<script type="math/tex; mode=display">f(x) \approx f(x_0) + f'(x_0)(x - x_0)</script>

<p>Let’s visualize what this formula means and how it can be useful to find a good direction. The generalization of the tangent, which is a line, in a vector space is a hyperplane. In a 2D space it corresponds to a plane.</p>

<p><img src="/media/img/part1/tangent_space.png" alt="Taylor's theorem visualization" class="center-image modal-image" /></p>

<p>The plane in blue is the approximation given by Taylor’s theorem. The points in orange are the outputs of <script type="math/tex">f</script> for all points at a same distance <script type="math/tex">d</script> to <script type="math/tex">x_0</script>. In green, it’s the output of the point at distance <script type="math/tex">d</script> of <script type="math/tex">x_0</script> in the direction of the gradient and in blue, it’s the output of the point at distance $d$ in the opposite direction of the gradient.</p>

<p>You can clearly see that the direction toward which the plane goes up the quickest is the gradient <script type="math/tex">\frac{\partial f}{\partial x}(x_0)</script>. Conversely, the direction in which it goes down the quickest is the opposite of the gradient <script type="math/tex">-\frac{\partial f}{\partial x}(x_0)</script>.</p>

<p>If you are more a math person who likes to play with formulas, you can remark that <script type="math/tex">\frac{\partial f}{\partial x}(x_0)^T(x - x_0)</script> is an inner product. And, if we consider all points at the same distance <script type="math/tex">d</script> to <script type="math/tex">x_0</script>, the <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Cauchy-Schwarz inequality</a> tells us that this term is maximized when the point <script type="math/tex">(x - x_0)</script> has the same direction as the gradient and is minimized when the point has the opposite direction of the gradient.</p>

<p>Let’s go back to our goal to minimize the cost. Let <script type="math/tex">\theta_t</script> be the value of the parameter at iteration <script type="math/tex">t</script> of the algorithm. We can see <script type="math/tex">J</script> as a function of <script type="math/tex">\theta</script>, and we want to find a point around <script type="math/tex">\theta_t</script> that has a lower cost. Using our first order approximation of <script type="math/tex">J</script> around <script type="math/tex">\theta_t</script> we know that the direction toward which the cost is decreasing the most is <script type="math/tex">-\frac{\partial J}{\partial \theta}(\theta_t)</script>. Consequently, we will make a small step in this direction and choose the point:</p>

<script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \eta \frac{\partial J}{\partial \theta}(\theta_t)</script>

<p>where <script type="math/tex">\eta</script> is called the <em>learning rate</em>. It is a parameter that controls the size of the steps we make. We will dig deeper into optimization algorithms later in another part.</p>

<p>You can see gradient descent in action, in the animation below.</p>

<p><img src="/media/img/part1/gradient_descent.gif" alt="Gradient descent in action" class="center-image modal-image" /></p>

<p>You can get the code of these two simulations <a href="https://github.com/pvigier/gradient-descent">here</a>.</p>

<p>I stop there for the brief introduction to machine learning. We will now see how to model some parametric functions and how they relate to neural networks.</p>

<h1 id="computational-graphs">Computational graphs</h1>

<p>A <em>computational graph</em> is a graph which represents a computation.</p>

<p>Let’s see some examples to show how this tool can be used to model useful functions.</p>

<p>Imagine, you want to fit very simple data like in the figure below.</p>

<p><img src="/media/img/part1/linear_regression.png" alt="Linear regression" class="center-image modal-image" /></p>

<p>The natural idea is to use a function like this one <script type="math/tex">f_{\theta} : x \mapsto ax + b</script> where <script type="math/tex">\theta = (a, b)</script> is the parameter of the function. This function can be represented with the following computational graph.</p>

<p><img src="/media/img/part1/linear_regression_graph.svg" alt="Computational graph for the linear regression" class="center-image modal-image" /></p>

<p>Now, imagine you have to solve a more difficult problem and you want to use a neural network, like the one below.</p>

<p><img src="/media/img/part1/network_3_5_3.svg" alt="Neural networks 3-5-3" class="center-image modal-image" /></p>

<p>It has two layers, the hidden layer uses <script type="math/tex">\tanh</script> as activation function and the output layer uses the sigmoid <script type="math/tex">\sigma : t \mapsto \frac{1}{1 + \exp(-t)}</script>.</p>

<p>This model is strictly equivalent to the function <script type="math/tex">f_{\theta} : x \mapsto \sigma(\tanh(xW_1)W_2)</script> where <script type="math/tex">W_1</script> are the weights of the first layer and <script type="math/tex">W_2</script> the weights of the second layer. In this case again, <script type="math/tex">f</script> is a parametric function which depends on <script type="math/tex">\theta = (W_1, W_2)</script>.</p>

<p>The following computational graph corresponds to the neural network depicted above.</p>

<p><img src="/media/img/part1/graph_3_5_3.svg" alt="Computational graph for the neural networks" class="center-image modal-image" /></p>

<p>We can see several things from these examples. Firstly there are different nodes:</p>
<ul>
  <li>Input nodes (in rose): they represent the inputs of the function. In our examples, they are the <script type="math/tex">x</script> nodes.</li>
  <li>Operation nodes (in blue): they are nodes that represent operations. They take inputs, make a computation and give ouputs. In our examples, the <script type="math/tex">\sigma</script>, <script type="math/tex">\tanh</script>, <script type="math/tex">\times</script>, <script type="math/tex">+</script> nodes are operation nodes.</li>
  <li>Parameter nodes (in green): they represent the parameters of the function. In our example, they are <script type="math/tex">a</script>, <script type="math/tex">b</script>, <script type="math/tex">W_1</script> and <script type="math/tex">W_2</script>.</li>
</ul>

<p>Secondly, it allows a more compact notation, instead of drawing lots of neurons, we can only draw few nodes to represent a whole network.</p>

<p>Finally, we can model a broader class of parametric functions. We are not limited at all by the biological inspiration. You can unleash your engineering spirit and craft all sort of functions.</p>

<p>In the next sections, we will explain how to code a computational graph to model a parametric function and how to optimize them to fit your data.</p>

<h1 id="architecture-of-the-library">Architecture of the library</h1>

<p>First, I am going to present the architecture of the library. Then we are going to explain the code.</p>

<p>There are three main classes that we are going to code in this part: the Node class, the Graph class and the OptimizationAlgorithm class.</p>

<p>The Node class contains most of the logic of the library. It’s the hard part.</p>

<p>The Graph class is a container that contains nodes and allows to interact easily with them.</p>

<p>Finally, the OptimizationAlgorithm class implements an optimization algorithm like the gradient descent algorithm we have described above. The class uses the parameter nodes and optimizes their value to minimize the cost. This class will allow us to easily switch between different algorithms. If you are familiar with design patterns, you should have recognized the <a href="https://en.wikipedia.org/wiki/Strategy_pattern">strategy pattern</a>.</p>

<p>The UML diagram below gives a global view of the classes and their interactions.</p>

<p><img src="/media/img/part1/uml_diagram.svg" alt="UML diagram of the library" class="center-image modal-image" /></p>

<p>And before, I forget you can retrieve the full code for this chapter <a href="https://github.com/pvigier/pychain-part1">here</a>.</p>

<h1 id="node-class">Node class</h1>

<p>A node have several inputs and several outputs. During the propagation, a node uses its inputs to compute its ouputs as depicted below.</p>

<p><img src="/media/img/part1/node_propagation.svg" alt="Propagation in a node" class="center-image modal-image" /></p>

<p>The goal of the backpropagation is to compute the derivative of the cost with respect to the parameters to use the gradient descent algorithm. To do that, we are going to use the <em>chain rule</em>, a lot. Indeed, thanks to the chain rule, it is possible to express the derivative of the cost with respect to an input of a node with the derivatives of the cost with respect to the outputs of the same node.</p>

<p><img src="/media/img/part1/node_backpropagation.svg" alt="Backpropagation in a node" class="center-image modal-image" /></p>

<p>Precisely, if a node has <script type="math/tex">m</script> inputs and <script type="math/tex">n</script> outputs, we have:</p>

<script type="math/tex; mode=display">\forall i \in \{1, ..., m\}, \frac{\partial J}{\partial x_i} = \sum_{j=1}^{n}{\frac{\partial y_j}{\partial x_i}\frac{\partial J}{\partial y_j}}</script>

<p>And the term <script type="math/tex">\frac{\partial y_j}{\partial x_i}</script> only depends of the nature of the node.</p>

<p>Consequently, we are going to backpropagate the derivatives from the “end” of the graph to the parameter nodes.</p>

<p>An illustration with the neural networks shown before:</p>

<p><img src="/media/img/part1/graph_backpropagation.svg" alt="Backpropagation in a graph" class="center-image modal-image" /></p>

<h2 id="base-class">Base class</h2>

<p>Most of the code is contained in the base class. We are going to divide the work in two stages. First, we are going to see the methods that are useful to create the computational graph. Then we are going to describe the methods used during the propagation and the backpropagation.</p>

<p>Let’s see the first part of the code.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Node</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parents</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">nb_outputs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="c"># Parents for each input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_parents</span><span class="p">(</span><span class="n">parents</span> <span class="ow">or</span> <span class="p">[])</span>
        <span class="c"># Children for each output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">children</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_outputs</span><span class="p">)]</span>

        <span class="c"># Memoization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dJdx</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dJdy</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="c"># Dirty flags</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dirty</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_dirty</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="k">def</span> <span class="nf">set_parents</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parents</span><span class="p">):</span>
        <span class="c"># Fill self.parents</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parents</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i_input</span><span class="p">,</span> <span class="p">(</span><span class="n">parent</span><span class="p">,</span> <span class="n">i_parent_output</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parents</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">parent</span><span class="p">,</span> <span class="n">i_parent_output</span><span class="p">))</span>
            <span class="n">parent</span><span class="o">.</span><span class="n">add_child</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i_input</span><span class="p">,</span> <span class="n">i_parent_output</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">add_child</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">child</span><span class="p">,</span> <span class="n">i_child_input</span><span class="p">,</span> <span class="n">i_output</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="n">i_output</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">child</span><span class="p">,</span> <span class="n">i_child_input</span><span class="p">))</span></code></pre></figure>

<p>A node has a list of its parents, the list have the following form <code class="highlighter-rouge">[(node1, i_output1), (node2, i_output2), ... ]</code>. For each parent node, we precise the index of its output to which the current node is connected. The first couple corresponds to the first input of the node <script type="math/tex">x_1</script>, the second to <script type="math/tex">x_2</script>, etc.</p>

<p>A node has a 2D list of its children too. The first dimension is the output index. And for each output index, there is a list containing the nodes connected to this output. The list has the following form <code class="highlighter-rouge">[(node1, i_input1), (node2, i_input2), ...]</code>. For each child node, we precise the index of its input that is connected to this output.</p>

<p>OK, it is a bit hard to follow. Hopefully, a diagram will make it crystal clear!</p>

<p><img src="/media/img/part1/node_links.svg" alt="Node links" class="center-image modal-image" /></p>

<p>We can set the parents during the initialization or later by using <code class="highlighter-rouge">set_parents</code>. We have not to manually set the children.</p>

<p>That’s all for the dependencies between nodes.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Node</span><span class="p">:</span>
   <span class="c"># ...</span>

   <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i_output</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dirty</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">parent</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="n">i_parent_output</span><span class="p">)</span> \
                <span class="k">for</span> <span class="p">(</span><span class="n">parent</span><span class="p">,</span> <span class="n">i_parent_output</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_output</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_dirty</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">i_output</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">compute_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i_input</span><span class="p">):</span>
        <span class="c"># If there are no children, return zero</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">children</span><span class="p">)</span> <span class="k">for</span> <span class="n">children</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="n">i_input</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="c"># Get gradient with respect to the i_inputth input</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_dirty</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dJdy</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">get_gradient</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> \
                <span class="k">for</span> <span class="n">child</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">children</span><span class="p">)</span> <span class="k">for</span> <span class="n">children</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dJdx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradient</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gradient_dirty</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dJdx</span><span class="p">[</span><span class="n">i_input</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_memoization</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c"># Reset flags</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dirty</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_dirty</span> <span class="o">=</span> <span class="bp">True</span></code></pre></figure>

<p>The implementation of propagation and backpropagation is very symmetrical.</p>

<p>The method <code class="highlighter-rouge">get_output</code> asks the parents of the node their outputs and save them in <code class="highlighter-rouge">self.x</code>. Then we use, <code class="highlighter-rouge">compute_output</code> and we save the result in <code class="highlighter-rouge">self.y</code>. Finally, we set the dirty flag to false. We use <a href="https://en.wikipedia.org/wiki/Memoization">memoization</a> not to compute twice the same value. It is very important because each child will ask the node for outputs, and recursively this node will ask its parents for their outputs, etc. If we do not use memoization, we would waste a lot of time doing unnecessary computations.</p>

<p>The method <code class="highlighter-rouge">compute_output</code> is abstract, it will be implemented in specialized classes.</p>

<p><code class="highlighter-rouge">get_gradient</code> is very similar to <code class="highlighter-rouge">get_output</code>, for each output we gather the derivative of the cost with respect to this output and save it in <code class="highlighter-rouge">self.dJdy</code>. Then we use <code class="highlighter-rouge">compute_gradient</code> to compute <code class="highlighter-rouge">self.dJdx</code> and set the dirty flag to false.</p>

<p>Finally, there is the method <code class="highlighter-rouge">reset_memoization</code> to set the flags to true.</p>

<p>In the next subsections, we are going to see the specializations of this class.</p>

<h2 id="input-nodes">Input nodes</h2>

<p>The class <code class="highlighter-rouge">InputNode</code> has an attribute <code class="highlighter-rouge">value</code> that can be set. The function <code class="highlighter-rouge">get_output</code> is overloaded to directly return this value.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">InputNode</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">Node</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">set_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span>

    <span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">dJdy</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span></code></pre></figure>

<h2 id="parameter-nodes">Parameter nodes</h2>

<p>The class <code class="highlighter-rouge">ParameterNode</code> has a new parameter <code class="highlighter-rouge">w</code>. We use the letter <code class="highlighter-rouge">w</code> to refer to the weights, like in neural networks.</p>

<p>The output of the node is simply the weight.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">ParameterNode</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
        <span class="n">Node</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">w</span>

    <span class="k">def</span> <span class="nf">compute_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">dJdy</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span></code></pre></figure>

<p>This class is very simple. The only issue is that the constructor expects an initial value for the weights. There are several thumb rules regarding the initialization of the weights. The general idea is to sample from a zero mean distribution with a small standard deviation. You can see more details about initialization in <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Efficient BackProp</a>.</p>

<p>You will see several examples of initialization later in the tutorial.</p>

<h2 id="operation-nodes">Operation nodes</h2>

<p>Then there are the operation nodes. There is a lot of implemented nodes, I am not going to describe the code for each of them.</p>

<p>Let’s take the example of the <code class="highlighter-rouge">SigmoidNode</code> which implements the function <script type="math/tex">\sigma</script> element-wise applied.</p>

<p>We only plug the formulas for the output and the gradient in the methods <code class="highlighter-rouge">get_output</code> and <code class="highlighter-rouge">get_gradient</code>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">SigmoidNode</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">compute_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span>

    <span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">dJdy</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span></code></pre></figure>

<p>To have decent performances in Python, it is very important to use numpy at most and to forbid for-loops. Consequently, it is important to express the formulas in term of matrix operations.</p>

<p>If you want to see all the formulas and how to derive the backpropagation formulas, you can go to the appendix on matrix derivations.</p>

<h2 id="gradient-nodes">Gradient nodes</h2>

<p>The last type of node is the <code class="highlighter-rouge">GradientNode</code>. I have never spoken about gradient node yet. There are very similar to input nodes but this time, it is the value of the gradient that can be set.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">GradientNode</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parents</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">Node</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parents</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">set_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">compute_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span>

    <span class="k">def</span> <span class="nf">get_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i_input</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span></code></pre></figure>

<p>During backpropagation, if every node asks their children for gradient, to whom the “last” node asks its gradient with respect to its output? Gradient nodes are the answer to this question. We can set the gradient to be backpropagated.</p>

<p>There are two ways of doing this. We can just add a gradient node as a child to the output node of the graph. Then, we set manually the value of the gradient node to the derivative of the cost with respect to the output of the graph.</p>

<p>Another way of proceeding that I prefer is to add the cost function directly in the computational graph. Then, we just have to add a gradient node with a value of <script type="math/tex">1</script> as a child of the cost node. And that’s it!</p>

<p>Consider the graph representing the neural networks described above. If we had an input node <script type="math/tex">y</script> to represent the expected output and we choose the squared error as cost function, we obtain this new graph:</p>

<p><img src="/media/img/part1/graph_with_cost.svg" alt="A graph with cost inside it" class="center-image modal-image" /></p>

<p>The yellow node at the end is a gradient node which always returns 1 as gradient.</p>

<h1 id="graph-class">Graph class</h1>

<p>The <code class="highlighter-rouge">Graph</code> class is very simple.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Graph</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nodes</span><span class="p">,</span> <span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">expected_output_nodes</span><span class="p">,</span> <span class="n">cost_node</span><span class="p">,</span> <span class="n">parameter_nodes</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nodes</span> <span class="o">=</span> <span class="n">nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_nodes</span> <span class="o">=</span> <span class="n">input_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_nodes</span> <span class="o">=</span> <span class="n">output_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expected_output_nodes</span> <span class="o">=</span> <span class="n">expected_output_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cost_node</span> <span class="o">=</span> <span class="n">cost_node</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parameter_nodes</span> <span class="o">=</span> <span class="n">parameter_nodes</span>

        <span class="c"># Create a gradient node</span>
        <span class="n">GradientNode</span><span class="p">([(</span><span class="n">cost_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">propagate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_memoization</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_nodes</span><span class="p">):</span>
            <span class="n">node</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_nodes</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">backpropagate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">y</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">expected_output_nodes</span><span class="p">):</span>
            <span class="n">node</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost_node</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameter_nodes</span><span class="p">:</span>
            <span class="n">node</span><span class="o">.</span><span class="n">get_gradient</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cost</span>

    <span class="k">def</span> <span class="nf">reset_memoization</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
            <span class="n">node</span><span class="o">.</span><span class="n">reset_memoization</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_parameter_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameter_nodes</span></code></pre></figure>

<p>It takes the list of all the nodes in the graph. In addition, it needs to know the function of several nodes in the graph: the inputs, the ouputs, the expected outputs and the cost.</p>

<p>There are also two useful functions:</p>
<ul>
  <li><code class="highlighter-rouge">propagate</code>: to compute the outputs of the graph ;</li>
  <li><code class="highlighter-rouge">backpropagate</code>: to accumulate the gradient of the cost with respect to the parameters.</li>
</ul>

<p>Finally, the method <code class="highlighter-rouge">reset_memoization</code> resets the flags for all the nodes and <code class="highlighter-rouge">get_parameter_nodes</code> returns the parameter nodes. The latter method is useful to give to the optimization algorithm the nodes for which it has to optimize the weights.</p>

<h1 id="optimization-algorithms">Optimization algorithms</h1>

<p>The last class we have to write before we can test our library is the <code class="highlighter-rouge">OptimizationAlgorithm</code> class.</p>

<p>The class is a generic template for all first-order iterative optimization algorithms such as gradient descent.</p>

<p>For this kind of algorithm at each step <script type="math/tex">t</script>, we compute a direction <script type="math/tex">v_t</script> toward which we should move the parameter <script type="math/tex">\theta_t</script>. We have:</p>

<script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \eta v_t</script>

<p>Where <script type="math/tex">\eta</script> is the learning rate.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">OptimizationAlgorithm</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameter_nodes</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parameter_nodes</span> <span class="o">=</span> <span class="n">parameter_nodes</span>

        <span class="c"># Parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameter_nodes</span><span class="p">):</span>
            <span class="n">direction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_direction</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">get_gradient</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="n">node</span><span class="o">.</span><span class="n">w</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">direction</span>

    <span class="k">def</span> <span class="nf">compute_direction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">()</span></code></pre></figure>

<p>For gradient descent as explained before, we simply have <script type="math/tex">v_t = \frac{\partial J}{\partial \theta}(\theta_t)</script>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">GradientDescent</span><span class="p">(</span><span class="n">OptimizationAlgorithm</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameter_nodes</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="n">OptimizationAlgorithm</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameter_nodes</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">compute_direction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">grad</span></code></pre></figure>

<h1 id="some-examples">Some examples</h1>

<p>Now we have a fully functional library. The last missing piece is to create a computational graph and try it!</p>

<p>We are going to use fully-connected neural networks to learn two functions respectively XOR and a disk.</p>

<p>We will use the function <code class="highlighter-rouge">fully_connected</code> which takes as input a list of the sizes of the layers and returns a computational graph which represents such a network.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">create_fully_connected_network</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">parameter_nodes</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c"># Input</span>
    <span class="n">input_node</span> <span class="o">=</span> <span class="n">InputNode</span><span class="p">()</span>
    <span class="n">nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_node</span><span class="p">)</span>

    <span class="n">cur_input_node</span> <span class="o">=</span> <span class="n">input_node</span>
    <span class="n">prev_size</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
        <span class="c"># Create a layer</span>
        <span class="n">bias_node</span> <span class="o">=</span> <span class="n">AddBiasNode</span><span class="p">([(</span><span class="n">cur_input_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)])</span>
        <span class="n">parameter_node</span> <span class="o">=</span> <span class="n">ParameterNode</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">prev_size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">prod_node</span> <span class="o">=</span> <span class="n">MultiplicationNode</span><span class="p">([(</span><span class="n">bias_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">parameter_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)])</span>
        <span class="c"># Activation function for hidden layers</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
            <span class="c"># Tanh</span>
            <span class="n">activation_node</span> <span class="o">=</span> <span class="n">TanhNode</span><span class="p">([(</span><span class="n">prod_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)])</span>
            <span class="c"># Relu</span>
            <span class="c">#activation_node = ReluNode([(prod_node, 0)])</span>
        <span class="c"># Activation function for the output layer</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">activation_node</span> <span class="o">=</span> <span class="n">SigmoidNode</span><span class="p">([(</span><span class="n">prod_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)])</span>
        <span class="c"># Save the new nodes</span>
        <span class="n">parameter_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parameter_node</span><span class="p">)</span>
        <span class="n">nodes</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias_node</span><span class="p">,</span> <span class="n">parameter_node</span><span class="p">,</span> <span class="n">prod_node</span><span class="p">,</span> <span class="n">activation_node</span><span class="p">]</span>
        <span class="n">cur_input_node</span> <span class="o">=</span> <span class="n">activation_node</span>
        <span class="n">prev_size</span> <span class="o">=</span> <span class="n">size</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="c"># Expected output</span>
    <span class="n">expected_output_node</span> <span class="o">=</span> <span class="n">InputNode</span><span class="p">()</span>
    <span class="c"># Cost function</span>
    <span class="n">cost_node</span> <span class="o">=</span> <span class="n">BinaryCrossEntropyNode</span><span class="p">([(</span><span class="n">expected_output_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">cur_input_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)])</span>

    <span class="n">nodes</span> <span class="o">+=</span> <span class="p">[</span><span class="n">expected_output_node</span><span class="p">,</span> <span class="n">cost_node</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">Graph</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="p">[</span><span class="n">input_node</span><span class="p">],</span> <span class="p">[</span><span class="n">cur_input_node</span><span class="p">],</span> <span class="p">[</span><span class="n">expected_output_node</span><span class="p">],</span> <span class="n">cost_node</span><span class="p">,</span> <span class="n">parameter_nodes</span><span class="p">)</span></code></pre></figure>

<p>The function is a bit long but simple, we simply create nodes and connect them. First, we create an input node. Then, for each layer, we create a bias node, a parameter node, a multiplication node and an activation node. These nodes model the operation <script type="math/tex">x \mapsto f((1 \mid x)W)</script> where <script type="math/tex">(1 \mid x)</script> denotes the matrix <script type="math/tex">x</script> to which we have added a column of 1 at the beginning, <script type="math/tex">W</script> the weights of the parameter node and <script type="math/tex">f</script> the activation function.</p>

<p>We use two different activation functions: <script type="math/tex">\tanh</script> and <script type="math/tex">\sigma</script>. <script type="math/tex">\tanh</script> is used in the hidden layers, it is a nonlinear function which maps <script type="math/tex">\mathbb{R}</script> to <script type="math/tex">]-1, 1[</script>. It is very important that activation functions are nonlinear otherwise the multilayer neural network is equivalent to a one layer neural network.</p>

<p>We use the sigmoid function as activation function for the output layer because it maps <script type="math/tex">\mathbb{R}</script> to <script type="math/tex">]0,1[</script> and consequently the output of the network can be interpreted as a probability, usually <script type="math/tex">p(y=1 \mid x)</script>.</p>

<p>For the cost function, we choose the binary cross entropy. If you want to know more about the output functions and the cost functions, you can read the appendix.</p>

<h2 id="xor">XOR</h2>

<p>Learning XOR is an interesting problem because XOR is surely the simplest boolean function that cannot be learnt by a linear classifier. Crafted features or a deep neural network are necessary to solve this nonlinear problem.</p>

<p>Let’s use this piece of code to train the network.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># Create the graph and initialize the optimization algorithm</span>
<span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">create_fully_connected_network</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
<span class="n">sgd</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_parameter_nodes</span><span class="p">(),</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="c"># Train</span>
<span class="n">t</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">nb_passes</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="k">for</span> <span class="n">i_pass</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_passes</span><span class="p">):</span>
    <span class="c"># Propagate, backpropagate and optimize</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">propagate</span><span class="p">([</span><span class="n">X</span><span class="p">])</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">backpropagate</span><span class="p">([</span><span class="n">Y</span><span class="p">])</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">sgd</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c"># Save the cost</span>
    <span class="n">t</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i_pass</span><span class="p">)</span>
    <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span></code></pre></figure>

<p>Finally, you can use the <code class="highlighter-rouge">visualize</code> to compute the output of the graph for every <script type="math/tex">x \in [-0.5, 1.5]^2</script> and see the frontier. You can see that the network models a nonlinear function.</p>

<p>Below, you can see such generated images for two different activation functions for the hidden layers. We can note that <script type="math/tex">\tanh</script> seems to create smooth frontiers while <script type="math/tex">ReLU</script> creates sharper ones.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">With tanh</th>
      <th style="text-align: center">With ReLU</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/media/img/part1/xor_4_4_1_tanh.png" alt="XOR with tanh as activation function" class="center-image modal-image" /></td>
      <td style="text-align: center"><img src="/media/img/part1/xor_4_4_1_relu.png" alt="XOR with ReLU as activation function" class="center-image modal-image" /></td>
    </tr>
  </tbody>
</table>

<h2 id="disk">Disk</h2>

<p>Just for fun let’s learn another function and generate beautiful images. The function we want to learn is such that <script type="math/tex">f(x) = 1</script> if <script type="math/tex">x</script> is inside the disk of radius 0.5 and centered on (0.5, 0.5) and <script type="math/tex">f(x) = 0</script> otherwise.</p>

<p>The function is clearly nonlinear and consequently, it is a perfect problem for a neural network.</p>

<p>To test the code for the disk function, you just have to comment/uncomment few lines in train.py. Below, there are some outputs.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">With tanh</th>
      <th style="text-align: center">With ReLU</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/media/img/part1/disk_4_4_1_tanh.png" alt="XOR with tanh as activation function" class="center-image modal-image" /></td>
      <td style="text-align: center"><img src="/media/img/part1/disk_4_4_1_relu.png" alt="XOR with ReLU as activation function" class="center-image modal-image" /></td>
    </tr>
  </tbody>
</table>

<h1 id="to-go-further">To go further</h1>

<p>Some problems to sharpen your understanding of neural networks:</p>
<ul>
  <li>What is the smallest number of layers and neurons necessary to perfectly learn the XOR function? Verify your answer by experience.
<a href="#clue1" data-toggle="collapse">Clues</a></li>
</ul>
<div id="clue1" class="collapse">
<p>As mentionned before, one layer (only the output layer) is not sufficient because the frontier would be a straight line. So we should take at least two layers. Each layer should have at least two neurons otherwise the frontier is again a straight line. A network with 2 layers and 2 neurons is the smallest network able to learn the XOR function. You could try to find acceptable weights by hand or you can use gradient descent!</p>
</div>
<ul>
  <li>Study the influence of the learning rate.
<a href="#clue2" data-toggle="collapse">Clues</a></li>
</ul>
<div id="clue2" class="collapse">
<p>Try very small values, values around 1 and very high values.</p>

<p>You can see on the curves below that the cost tends to zero for learning rates small enough. And the larger the learning rate is, the faster the cost tends to zero. However for too large learning rates the cost does not tend to zero anymore.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Convergence</th>
      <th style="text-align: center">Divergence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/media/img/part1/learning_rates_convergence.png" alt="Learning curves for low learning rates" class="center-image" /></td>
      <td style="text-align: center"><img src="/media/img/part1/learning_rates_divergence.png" alt="Learning curves for high learning rates" class="center-image" /></td>
    </tr>
  </tbody>
</table>

</div>
<ul>
  <li>Try to create other datasets using other functions and learn them using a computational graph.</li>
</ul>

    
	<p>Tags: <span class="label label-primary"><a href="/tag/math">math</a></span> <span class="label label-primary"><a href="/tag/python">python</a></span> </p>
	
</div><!-- /.blog-post -->


<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://pvigier-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

</div><!-- /.blog-main -->
<div class="col-sm-3 col-sm-offset-1 blog-sidebar">
	<div class="sidebar-module">
        <h4>Tags</h4>
        <ol class="list-unstyled">
		
			<li><a href="/tag/math">math (3)</a></li>
		
			<li><a href="/tag/python">python (6)</a></li>
		
			<li><a href="/tag/cpp">cpp (4)</a></li>
		
			<li><a href="/tag/pcg">pcg (3)</a></li>
		
			<li><a href="/tag/simulopolis">simulopolis (4)</a></li>
		
			<li><a href="/tag/linux">linux (1)</a></li>
		
			<li><a href="/tag/geometry">geometry (1)</a></li>
		
        </ol>
    </div>
    <div class="sidebar-module">
        <h4>Archives</h4>
        <ol class="list-unstyled">
		
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
			<li id="yNovember 2018"><a href="/2018/11">November 2018 (3)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
        
            
            
            
            
			<li id="yOctober 2018"><a href="/2018/10">October 2018 (3)</a></li>
				
            
        
            
            
            
            
			<li id="yJune 2018"><a href="/2018/06">June 2018 (1)</a></li>
				
            
        
            
            
            
            
			<li id="yMay 2018"><a href="/2018/05">May 2018 (1)</a></li>
				
            
        
            
            
            
            
        
            
            
            
            
			<li id="yFebruary 2018"><a href="/2018/02">February 2018 (2)</a></li>
				
            
        
            
            
            
            
			<li id="yAugust 2017"><a href="/2017/08">August 2017 (1)</a></li>
				
            
        
            
            
            
            
			<li id="yJuly 2017"><a href="/2017/07">July 2017 (1)</a></li>
				
            
        
            
            
            
            
			<li id="yFebruary 2017"><a href="/2017/02">February 2017 (1)</a></li>
				
            
        
        </ol>
    </div>
    <div class="sidebar-module">
        <h4>Follow me</h4>
        <ol class="list-unstyled">
            <li><a href="https://github.com/pvigier">GitHub</a></li>
            <li><a href="https://pvigier.itch.io/">itch.io</a></li>
            <li><a href="https://twitter.com/PierreVigier">Twitter</a></li>
            <li><a href="/rss.xml">RSS</a></li>
        </ol>
    </div>
</div><!-- /.blog-sidebar -->

            </div><!-- /.row -->
        </div><!-- /.container -->

        <footer class="blog-footer">
            <p>Powered by <a href="http://getbootstrap.com">Bootstrap</a> and <a href="http://jekyllrb.com">Jekyll</a>.</p>
            <p>
                <a href="#">Back to top</a>
            </p>
        </footer>
        <!-- Javascript -->
        <!-- jQuery -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
        <!-- Bootstrap -->
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
        <!-- MathJax -->
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
        <!-- Analytics -->
        <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-30902264-4', 'auto');
        ga('send', 'pageview');
        </script>
        <!-- Modal images -->
        <script src="/media/js/modal.js"></script>
    </body>
</html>
